@inproceedings{07Liu24,
author = {David Liu and Jonathan Calver and Michelle Craig},
title = {Are a Static Analysis Tool Study's Findings Static? A Replication},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653545},
doi = {10.1145/3649217.3653545},
abstract = {In 2017, Edwards et al. studied a large corpus of Java programs collected through an automated submission and assessment system that integrated static analysis feedback. They found that errors reported were most commonly related to formatting, but that the frequency of errors they categorized as "Coding Flaws" correlated with program correctness grades. They argued that static analysis feedback could detect problems relating to code correctness and could therefore be useful beyond evaluating conformance to style rules, but that students may overlook non-cosmetic error messages because of the relative volume of formatting errors. In this paper we perform a conceptual replication of the Edwards et al. study with 1270 CS1 students learning Python. We confirm that almost a decade later and even after being instructed to use the auto-formatting options within their IDE, students still encounter mostly formatting errors when using a static analysis tool. We find that the second- most common category of errors detected are "Coding Flaws", and, like Edwards et al., that the frequency of coding flaws identified by the static analysis tool correlates to program correctness. When we examine trends based on levels of prior programming experience, we find that all students tend to make more formatting errors than other kinds of errors, but that students with no prior programming experience have more errors reported across all error categories.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {80–86},
numpages = {7},
keywords = {error messages, replication, static analysis, style checker},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

inproceedings{03Horton23,
author = {Diane Horton and David Liu and Sheila A. McIlraith and Nina Wang},
title = {Is More Better When Embedding Ethics in CS Courses?},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569881},
doi = {10.1145/3545945.3569881},
abstract = {Embedding ethics modules in computer science (CS) courses is an approach to post-secondary ethics education that has been gaining traction. In contrast to dedicated courses on ethics in CS, embedding ethics modules into CS courses supports tight connections between ethical considerations and CS concepts, as well as enabling repeated exposure to ethics across multiple courses. Initial studies of the effectiveness of such modules suggest that this approach can increase both student interest in ethics and technology, and student self-efficacy towards incorporating ethical considerations in their computing work. Departments wishing to deploy embedded ethics (EE) modules need to decide how to invest resources, including class time, to maximize effectiveness while maintaining curriculum objectives. Such considerations include the number of EE module experiences a student has throughout their degree program, as well as the spacing of those experiences.Research to date has focused on the effect of a single embedded ethics module. In this paper, we report on a study examining the impact of experiencing EE modules in multiple courses. Among our findings, our results suggest that more is not necessarily better --- that a modest number of periodic exposures to EE modules over the course of a degree
program may be sufficient to achieve sustained positive attitudes and self-efficacy among students. While a picture is beginning to emerge, these results highlight the need for further research on the effectiveness of embedded ethics programs as a whole.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {652–658},
numpages = {7},
keywords = {embedded ethics, ethics education, impact of technology on society},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{03Liu19,
 author = {David Liu and Andrew Petersen},
 title = {Static Analyses in Python Programming Courses},
 booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
 series = {SIGCSE '19},
 year = {2019},
 isbn = {978-1-4503-5890-3},
 location = {Minneapolis, MN, USA},
 pages = {666--671},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3287324.3287503},
 doi = {10.1145/3287324.3287503},
 acmid = {3287503},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cs1, error messages, errors, python, static analysis},
}

@inproceedings{03Liu24,
author = {David Liu and Jonathan Calver and Michelle Craig},
title = {A Static Analysis Tool in CS1: Student Usage and Perceptions of PythonTA},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636262},
doi = {10.1145/3636243.3636262},
abstract = {Static analysis tools help programmers write better code. In computer science education, such tools can help students identify common style and coding errors, and lead students to fixing them. However, static analysis tools should be deployed in the classroom with care, so that all students—especially novice programmers—are empowered to act on the feedback they receive from these tools. For the past several years, our department has been integrating PythonTA, an educational static analysis tool, into a large CS1 course to provide students regular formative feedback and as part of the grading of programming assignments. This paper reports on a study of over 800 students conducted in the September 2022 offering of this course. Using both quantitative and qualitative methods, we investigate how students used PythonTA and their perceptions of its helpfulness. Overall, students across all levels of prior programming experience report that this static analysis tool was helpful. Though students with prior experience reported being more confident using the tool than novice programmers, this gap in confidence shrank over the semester. A thematic analysis of student comments on PythonTA found that many students appreciated the tool for improving the quality of their code and their own programming habits, but others responded more negatively, including mentioning frustration or confusion caused by PythonTA’s error messages. We discuss our findings and provide recommendations for educators considering the adoption of static analysis tools in their classrooms.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {172–181},
numpages = {10},
keywords = {error messages, static analysis, style checker},
location = {<conf-loc>, <city>Sydney</city>, <state>NSW</state>, <country>Australia</country>, </conf-loc>},
series = {ACE '24}
}

@inproceedings{10.1145/3689187.3709615,
author = {Cruz Izu and Claudio Mirolo and J\"{u}rgen B\"{o}rstler and Harold Connamacher and Ryan Crosby and Richard Glassey and Georgiana Haldeman and Olli Kiljunen and Amruth N. Kumar and David Liu and Andrew Luxton-Reilly and Stephanos Matsumoto and Eduardo Carneiro de Oliveira and Se\'{A}n Russell and Anshul Shah},
title = {Introducing Code Quality at CS1 Level: Examples and Activities},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709615},
doi = {10.1145/3689187.3709615},
abstract = {Characterising code quality is a challenge that was addressed by a previous ITiCSE Working Group (B\"{o}rstler et al., 2017). As emerged from that study, educators, developers, and students have different perceptions of the aspects involved. The perception of code quality by CS1 students develops from the feedback they receive when submitting practical work. As a consequence of increasingly large classes and the widespread use of autograders, student code is predominantly assessed based on functional correctness, emphasising a machine-oriented perspective with scarce or no feedback given about human-oriented aspects of code quality. Such limited perception of code quality may negatively impact how students understand, create, and interact with code artefacts. Although B\"{o}rstler et al. concluded that "code quality should be discussed more thoroughly in educational programs", the lack of materials and time constraints have slowed down progress in that regard.The goal of this Working Group is to support CS1 instructors who want to introduce a broader perspective on code quality in their classroom, by providing a curated list of examples and activities suitable for novices. In order to achieve this goal, we have extracted from the CS education literature a range of examples and activities, which have then been analysed and organised in terms of code quality dimensions. We have also mapped the topics covered in those materials to existing taxonomies relevant to code quality in CS1. Based on this work, we provide: (1) a catalogue of examples that illustrates the range of quality defects that could be addressed at CS1 level; and (2) a sample set of activities devised to introduce code quality to CS1 students. These materials have the potential to help educators address the subject in more depth.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {339–377},
numpages = {39},
keywords = {CS1, activities, code quality, examples, readability, refactoring, style},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3641555.3704767,
author = {David Liu},
title = {Introducing PythonTA: A Suite of Code Analysis and Visualization Tools},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3704767},
doi = {10.1145/3641555.3704767},
abstract = {PythonTA is a free, open-source Python library consisting of a suite of educational code analysis tools designed to be easily added to course workflows, from being run locally by students to being integrated with autograding platforms. This tutorial will cover the three components of PythonTA: (1) a static code analyser that detects common issues related to code correctness and quality; (2) a dynamic verifier of program logical specifications (e.g., function type signatures and pre-/postconditions); and (3) a generator of multiple forms of program execution artifacts, including control flow graphs and memory state visualizations. Together, these tools are designed to empower students to gain deep understanding of the design and execution of their code and to provide them with beginner-friendly feedback on their code. The modular and configurable design of PythonTA makes it easy for educators to customize how they wish to use these tools in their classroom, and its distribution as a pure Python library makes it easy for students to install these tools on their computers for on-demand access. Participants in this tutorial will explore PythonTA through a series of hands-on demos and activities (all run locally on their laptops). They will learn about best practices in using PythonTA gathered from educators who have used these tools in their classrooms over the past eight years. Participants will leave the tutorial with concrete ideas on how to incorporate PythonTA in their teaching and with adaptable educational materials for doing so. A laptop is required for full participation in the tutorial.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1773},
numpages = {1},
keywords = {code comprehension, cs1, design by contract, python, static analysis, visualization},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}