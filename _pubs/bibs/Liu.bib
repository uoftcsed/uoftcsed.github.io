@inproceedings{07Liu24,
author = {David Liu and Jonathan Calver and Michelle Craig},
title = {Are a Static Analysis Tool Study's Findings Static? A Replication},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653545},
doi = {10.1145/3649217.3653545},
abstract = {In 2017, Edwards et al. studied a large corpus of Java programs collected through an automated submission and assessment system that integrated static analysis feedback. They found that errors reported were most commonly related to formatting, but that the frequency of errors they categorized as "Coding Flaws" correlated with program correctness grades. They argued that static analysis feedback could detect problems relating to code correctness and could therefore be useful beyond evaluating conformance to style rules, but that students may overlook non-cosmetic error messages because of the relative volume of formatting errors. In this paper we perform a conceptual replication of the Edwards et al. study with 1270 CS1 students learning Python. We confirm that almost a decade later and even after being instructed to use the auto-formatting options within their IDE, students still encounter mostly formatting errors when using a static analysis tool. We find that the second- most common category of errors detected are "Coding Flaws", and, like Edwards et al., that the frequency of coding flaws identified by the static analysis tool correlates to program correctness. When we examine trends based on levels of prior programming experience, we find that all students tend to make more formatting errors than other kinds of errors, but that students with no prior programming experience have more errors reported across all error categories.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {80–86},
numpages = {7},
keywords = {error messages, replication, static analysis, style checker},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

inproceedings{03Horton23,
author = {Horton, Diane and Liu, David and McIlraith, Sheila A. and Wang, Nina},
title = {Is More Better When Embedding Ethics in CS Courses?},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569881},
doi = {10.1145/3545945.3569881},
abstract = {Embedding ethics modules in computer science (CS) courses is an approach to post-secondary ethics education that has been gaining traction. In contrast to dedicated courses on ethics in CS, embedding ethics modules into CS courses supports tight connections between ethical considerations and CS concepts, as well as enabling repeated exposure to ethics across multiple courses. Initial studies of the effectiveness of such modules suggest that this approach can increase both student interest in ethics and technology, and student self-efficacy towards incorporating ethical considerations in their computing work. Departments wishing to deploy embedded ethics (EE) modules need to decide how to invest resources, including class time, to maximize effectiveness while maintaining curriculum objectives. Such considerations include the number of EE module experiences a student has throughout their degree program, as well as the spacing of those experiences.Research to date has focused on the effect of a single embedded ethics module. In this paper, we report on a study examining the impact of experiencing EE modules in multiple courses. Among our findings, our results suggest that more is not necessarily better --- that a modest number of periodic exposures to EE modules over the course of a degree
program may be sufficient to achieve sustained positive attitudes and self-efficacy among students. While a picture is beginning to emerge, these results highlight the need for further research on the effectiveness of embedded ethics programs as a whole.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {652–658},
numpages = {7},
keywords = {embedded ethics, ethics education, impact of technology on society},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{03Liu19,
 author = {David Liu and Andrew Petersen},
 title = {Static Analyses in Python Programming Courses},
 booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
 series = {SIGCSE '19},
 year = {2019},
 isbn = {978-1-4503-5890-3},
 location = {Minneapolis, MN, USA},
 pages = {666--671},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3287324.3287503},
 doi = {10.1145/3287324.3287503},
 acmid = {3287503},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cs1, error messages, errors, python, static analysis},
} 

@inproceedings{03Liu24,
author = {David Liu and Jonathan Calver and Michelle Craig},
title = {A Static Analysis Tool in CS1: Student Usage and Perceptions of PythonTA},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636262},
doi = {10.1145/3636243.3636262},
abstract = {Static analysis tools help programmers write better code. In computer science education, such tools can help students identify common style and coding errors, and lead students to fixing them. However, static analysis tools should be deployed in the classroom with care, so that all students—especially novice programmers—are empowered to act on the feedback they receive from these tools. For the past several years, our department has been integrating PythonTA, an educational static analysis tool, into a large CS1 course to provide students regular formative feedback and as part of the grading of programming assignments. This paper reports on a study of over 800 students conducted in the September 2022 offering of this course. Using both quantitative and qualitative methods, we investigate how students used PythonTA and their perceptions of its helpfulness. Overall, students across all levels of prior programming experience report that this static analysis tool was helpful. Though students with prior experience reported being more confident using the tool than novice programmers, this gap in confidence shrank over the semester. A thematic analysis of student comments on PythonTA found that many students appreciated the tool for improving the quality of their code and their own programming habits, but others responded more negatively, including mentioning frustration or confusion caused by PythonTA’s error messages. We discuss our findings and provide recommendations for educators considering the adoption of static analysis tools in their classrooms.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {172–181},
numpages = {10},
keywords = {error messages, static analysis, style checker},
location = {<conf-loc>, <city>Sydney</city>, <state>NSW</state>, <country>Australia</country>, </conf-loc>},
series = {ACE '24}
}
