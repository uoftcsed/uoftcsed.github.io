@inproceedings{07SuqingLiu24,
author = {Suqing Liu and Zezhu Yu and Feiran Huang and Yousef Bulbulia and Andreas Bergen and Michael Liut},
title = {Can Small Language Models With Retrieval-Augmented Generation Replace Large Language Models When Learning Computer Science?},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653554},
doi = {10.1145/3649217.3653554},
abstract = {Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system.To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s).We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {388–393},
numpages = {6},
keywords = {computing education, conversational agent, cs1, intelligence concentration, intelligent teaching assistant, intelligent tutoring system, large language models, locally deployable ai, personalized ai agent, retrieval augmented generation, small language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

inproceedings{03Zhang23,
author = {Zhang, Lisa and Simion, Bogdan and Kaler, Michael and Liaqat, Amna and Dick, Daniel and Bergen, Andi and Miljanovic, Michael and Petersen, Andrew},
title = {Embedding and Scaling Writing Instruction Across First- and Second-Year Computer Science Courses},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569729},
doi = {10.1145/3545945.3569729},
abstract = {Writing skills are often considered unimportant by computer science students and were under-emphasized in our curriculum. We describe our experience embedding CS-specific writing instruction at scale in most of our large, core, first- and second-year Computer Science courses, each with 300-800+ students. Our approach is to collaborate with a writing specialist and a community of course instructors, centralize the management of writing teaching assistants, and introduce a variety of relevant genres and contexts to help students develop and apply writing skills. We outline the institutional support and organization crucial to a project of this scale. In addition, we report on a survey collecting student perception of the writing instruction/assessment. We reflect on quantitative and qualitative evidence of success, as well as the challenges that we faced. We believe that many of these challenges will be common across institutions, parti
cularly those with large courses.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {610–616},
numpages = {7},
keywords = {wid, wac, wtl, written communication, cs education},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}
