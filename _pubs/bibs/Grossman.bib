inproceedings{05Kazemi24,
author = {Kazemitabaar, Majeed and Ye, Runlong and Wang, Xiaoning and Henley, Austin Zachary and Denny, Paul and Craig, Michelle and Grossman, Tovi},
title = {CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642773},
doi = {10.1145/3613904.3642773},
abstract = {Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student’s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI’s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {650},
numpages = {20},
keywords = {AI assistants, AI tutoring, class deployment, design guidelines, educational technology, generative AI, intelligent tutoring systems, large language models, programming education},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{01Kazemitabaar23,
author = {Majeed Kazemitabaar and Xinying Hou and Austin Henley and Barbara Jane Ericson and David Weintrop and Tovi Grossman},
title = {How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631806},
doi = {10.1145/3631802.3631806},
abstract = {As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners’ utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners’ use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {3},
numpages = {12},
keywords = {ChatGPT, Copilot, Introductory Programming, Large Language Models, OpenAI Codex, Self-paced Learning, Self-regulation},
location = {<conf-loc>, <city>Koli</city>, <country>Finland</country>, </conf-loc>},
series = {Koli Calling '23}
}

inproceedings{06Kazemitabaar22,
author = {Kazemitabaar, Majeed and Chyhir, Viktar and Weintrop, David and Grossman, Tovi},
title = {CodeStruct: Design and Evaluation of an Intermediary Programming Environment for Novices to Transition from Scratch to Python},
year = {2022},
isbn = {9781450391979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501712.3529733},
doi = {10.1145/3501712.3529733},
abstract = {Transitioning from block-based programming environments to conventional text-based programming languages is a challenge faced by many learners as they progress in their computer science education. In this paper, we introduce CodeStruct, a new intermediary programming environment for novices designed to support children who have prior experience with block-based programming to ease the eventual transition to text-based programming. We describe the development of CodeStruct and its key design features. We then present the results from a two-week long programming class with 26 high school students (ages 12-16; M=14 years) investigating how CodeStruct supported learners in transitioning from Scratch to Python. Our findings reveal how learners used the scaffolds designed into CodeStruct to support their transition from blocks to text, and that transitioning to CodeStruct reduced completion time (1.98x) and help requests (4.63x) when compared to transitioning directly to Python. Finally, learners that used CodeStruct, performed equally well (and slightly better in 10/16 programming activities) in their final transition to fully text-based Python programming.},
booktitle = {Interaction Design and Children},
pages = {261–273},
numpages = {13},
keywords = {blocks-to-text transition, block-based programming, high school computer science education},
location = {Braga, Portugal},
series = {IDC '22}
}

inproceedings{04Kazemitabaar23,
author = {Kazemitabaar, Majeed and Chow, Justin and Ma, Carl Ka To and Ericson, Barbara J. and Weintrop, David and Grossman, Tovi},
title = {Studying the Effect of AI Code Generators on Supporting Novice Learners in Introductory Programming},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580919},
doi = {10.1145/3544548.3580919},
abstract = {AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modification task. Our results show that using Codex significantly increased code-authoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual code-modification tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance. Of interest, learners with higher Scratch pre-test scores performed significantly better on retention post-tests, if they had prior access to Codex.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {455},
numpages = {23},
keywords = {Introductory Programming, AI Coding Assistants, ChatGPT, Large Language Models, GPT-3, AI-Assisted Pair-Programming, K-12 Computer Science Education, Copilot, OpenAI Codex},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{03Grossman23,
author = {Majeed Kazemitabaar and Viktar Chyhir and David Weintrop and Tovi Grossman},
title = {Scaffolding Progress: How Structured Editors Shape Novice Errors When Transitioning from Blocks to Text},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569723},
doi = {10.1145/3545945.3569723},
abstract = {Transitioning from block-based programming environments to text-based programming environments can be challenging as it requires students to learn new programming language concepts. In this paper, we identify and classify the issues encountered when transitioning from block-based to text-based programming. In particular, we investigate differences that emerge in learners when using a structured editor compared to an unstructured editor. We followed 26 high school students (ages 12-16; M=14 years) as they transitioned from Scratch to Python in three phases: (i) learning Scratch, (ii) transitioning from Scratch to Python using either a structured or unstructured editor, and (iii) evaluating Python coding skills using an unstructured editor. We identify 27 distinct types of issues and show that learners who used a structured editor during the transition phase had 4.6x less syntax issues and 1.9x less data-type issues compared to those who did not. When these learners switched to an unstructured editor for evaluation, they kept a lower rate on data-type issues but faced 4x more syntax errors.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {556–562},
numpages = {7},
keywords = {transition, structured editors, blocks-to-text, thematic analysis, high school programming, challenges, novices},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{05Joshi20,
author = {Nikhita Joshi and Justin Matejka and Fraser Anderson and Tovi Grossman and George Fitzmaurice},
title = {MicroMentor: Peer-to-Peer Software Help Sessions in Three Minutes or Less},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376230},
doi = {10.1145/3313831.3376230},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {quick help, one-on-one help, mentoring, software learning},
location = {Honolulu, HI, USA},
series = {CHI ’20}
}

@inproceedings{04Hudson18,
  author    = {Nathaniel Hudson and
               Benjamin J. Lafreniere and
               Parmit K. Chilana and
               Tovi Grossman},
  title     = {Investigating How Online Help and Learning Resources Support Children's
               Use of 3D Design Software},
  booktitle = {Proceedings of the 2018 {CHI} Conference on Human Factors in Computing
               Systems, {CHI} 2018, Montreal, QC, Canada, April 21-26, 2018},
  pages     = {257},
  year      = {2018},
  crossref  = {DBLP:conf/chi/2018},
  url       = {https://doi.org/10.1145/3173574.3173831},
  doi       = {10.1145/3173574.3173831},
  timestamp = {Wed, 21 Nov 2018 12:44:22 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/chi/HudsonLCG18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10WarnerLFG18,
  author    = {Jeremy Warner and
               Ben Lafreniere and
               George W. Fitzmaurice and
               Tovi Grossman},
  title     = {ElectroTutor: Test-Driven Physical Computing Tutorials},
  booktitle = {The 31st Annual {ACM} Symposium on User Interface Software and Technology,
               {UIST} 2018, Berlin, Germany, October 14-17, 2018},
  pages     = {435--446},
  year      = {2018},
  crossref  = {DBLP:conf/uist/2018},
  url       = {https://doi.org/10.1145/3242587.3242591},
  doi       = {10.1145/3242587.3242591},
  timestamp = {Wed, 21 Nov 2018 12:44:17 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/uist/WarnerLFG18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
