inproceedings{11Sibia25,
author = {Naaz Sibia and Jessica Wen and Amber Richardson and Yashika Jain and Angela Zavaleta Bernuy and Bogdan Simion and Andrew Petersen and Carolina Nobre and Michael Liut},
title = {From State to Structure: Towards Abstraction Support in CS2},
year = {2025},
isbn = {9798400715990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3769994.3769998},
doi = {10.1145/3769994.3769998},
abstract = {Motivation: CS2 students often struggle to connect low-level memory operations with high-level structural understanding, particularly with recursion and linked data structures. Visualizations can aid learning by making underlying relationships more visible and helping students track complex behavior. While such tools exist, most emphasize procedural flow without scaffolding abstraction. We explore whether multiple external representations (MERs), which are a well-established approach in other STEM fields that is underutilized in computing education, can support structural abstraction through synchronized, layered visualizations.Method: We used a tri-pane interface, grounded in Ainsworth’s DeFT framework, as a design probe. The system synchronized source code, memory diagrams, and abstract structural views. It was embedded into a 12-week CS2 course (n = 695) focused on recursive, reference-based structures. Students engaged with three MER-aligned activities during the semester. 440 completed an end-of-term survey, and 301 provided open-ended feedback. We analyzed perceived clarity, usability, and mental effort using validated scales and thematic coding.Results: Students described the MER visualizations as clear, intuitive, and helpful, especially for recursion and pointer-heavy logic. Many said the integration of views supported “seeing the bigger picture” and reduced reliance on rote tracing. Usability ratings were strong, mental effort was moderate, and non-native English speakers reported higher clarity than native speakers.Implications: As an initial exploration, these findings offer early empirical grounding for the further use of MERs in CS2. The work surfaces representational affordances, learner strategies, and design tensions that can inform future tools and evaluations of abstraction support in computing education.},
booktitle = {Proceedings of the 25th Koli Calling International Conference on Computing Education Research},
articleno = {17},
numpages = {12},
keywords = {CS2, Program Visualization, Multiple External Representations, Abstraction},
location = {
},
series = {Koli Calling '25}
}

@inproceedings{02Bulbulia25,
author = {Yousef Bulbulia and Ido Ben Haim and Jackson Lee and Brian Zhang and Daksh Malhotra and Andrew Petersen and Michael Liut},
title = {Codetierlist: Competitive Gamification's Impact on Self-Efficacy, Motivation, and Performance in Computing Education},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716646},
doi = {10.1145/3716640.3716646},
abstract = {Students struggle to understand why rigorous testing is necessary, often testing with a small number of examples and testing interactively instead of through a framework. Our goal is to encourage students to meaningfully engage in the testing process. We do so by developing a system, Codetierlist, that gamifies the process of testing on a programming assignment in a first-year programming course (CS2). Student tests for a programming assignment are run against both the instructor solution and other student solutions, and students receive feedback, in the form of a tier-based ranking, on how well their solution compares to fellow students within the shared student test suite. We compared the tests and assignment solutions students produced with and without Codetierlist. We also gathered student and instructor feedback on the experience of using the tool and measured student motivation and self-efficacy regarding testing. Students wrote more functionally correct code with Codetierlist, and they wrote significantly more and more precise tests with Codetierlist, even identifying previously unknown bugs in the instructor solution. We did not detect any changes to student self-efficacy, but students reported feeling more positive about testing and more motivated to test with Codetierlist. However, we also detected negative effects from the gamification method selected, as some students whose code was placed in a lower tier felt discouraged and less able to succeed. Additionally, we found that improvements to motivation and efficacy may vary based on a student’s prior experience. We provide the community with a tool, Codetierlist, for motivating students to engage more actively with testing in an assignment setting, and identify positive aspects of providing students with a target to test against. However, we reiterate the need for caution when introducing gamified elements that might be viewed as encouraging competition, as students who receive negative feedback from a comparison may feel unable to improve their situation.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {46–55},
numpages = {10},
keywords = {Testing, CS2, Programming, Gamification, Computing Education},
location = {
},
series = {ACE '25}
}


inproceedings{03Zavaleta25,
author = {Valeria Ramirez Osorio and Angela Zavaleta Bernuy and Bogdan Simion and Michael Liut},
title = {Understanding the Impact of Using Generative AI Tools in a Database Course},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701785},
doi = {10.1145/3641554.3701785},
abstract = {Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) have led to changes in educational practices by creating opportunities for personalized learning and immediate support. Computer science student perceptions and behaviors towards GenAI tools have been studied, but the effects of such tools on student learning have yet to be determined conclusively. We investigate the impact of GenAI tools on computing students' performance in a database course and aim to understand why students use GenAI tools in assignments. Our mixed-methods study (N=226) asked students to self-report whether they used a GenAI tool to complete a part of an assignment and why. Our results reveal that students utilizing GenAI tools performed better on the assignment part in which LLMs were permitted but did worse in other parts of the assignment and in the course overall. Also, those who did not use GenAI tools viewed more discussion board posts and participated more than those who used ChatGPT. This suggests that using GenAI tools may not lead to better skill development or mental models, at least not if the use of such tools is unsupervised, and that engagement with official course help supports may be affected. Further, our thematic analysis of reasons for using or not using GenAI tools, helps understand why students are drawn to these tools. Shedding light into such aspects empowers instructors to be proactive in how to encourage, supervise, and handle the use or integration of GenAI into courses, fostering good learning habits.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {959–965},
numpages = {7},
keywords = {computing education, databases, generative artificial intelligence, large language models, student behavior, student performance},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{01Miedema25,
author = {Daphne Miedema and Toni Taipalus and Vangel V. Ajanovski and Abdussalam Alawini and Martin Goodfellow and Michael Liut and Svetlana Peltsverger and Tiffany Young},
title = {Data Systems Education: Curriculum Recommendations, Course Syllabi, and Industry Needs},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709609},
doi = {10.1145/3689187.3709609},
abstract = {Data systems have been an important part of computing curricula for decades, and an integral part of data-focused industry roles such as software developers, data engineers, and data scientists. However, the field of data systems encompasses a large number of topics ranging from data manipulation and database distribution to creating data pipelines and data analytics solutions. Due to the slow nature of curriculum development, it remains unclear (i) which data systems topics are recommended across diverse higher education curriculum guidelines, (ii) which topics are taught in higher education data systems courses, and (iii) which data systems topics are actually valued in data-focused industry roles. In this study, we analyzed computing curriculum guidelines, course contents, and industry needs regarding data systems to uncover discrepancies between them. Our results show, for example, that topics such as data visualization, data warehousing, and semi-structured data models are valued in industry, yet seldom taught in courses. This work allows professionals to further align curriculum guidelines, higher education, and data systems industry to better prepare students for their working life by focusing on relevant skills in data systems education.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {95–123},
numpages = {29},
keywords = {curriculum, data engineering, data systems, database, education, industry, knowledge gap, skill set},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{11Sibia24,
author = {Naaz Sibia and Valeria Ramirez Osorio and Angela Zavaleta Bernuy and Efthimia Aivaloglou and Rutwa Engineer and Andrew Petersen and Michael Liut and Carolina Nobre},
title = {Exploring the Impact of Multiple Representations in Introductory Programming: A Pilot Study},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699587},
doi = {10.1145/3699538.3699587},
abstract = {This pilot study explores how visualization strategies, grounded in multiple representations theory, impact novice students’ engagement, and cognitive load during program tracing tasks. Students were were shown a visualization of the three-variable swap problem at the start of an introductory programming course (CS1) at a large public North American research-intensive university. We compared three conditions: interactive multiple representations, Python Tutor (a single-representation tool), and text-only methods. Preliminary results indicate that interactive multiple representations increase engagement for students with prior programming experience, while no significant differences were observed for students without prior experience. These findings suggest that while multiple representations may boost engagement, identifying how to effectively support students of all experience levels and reduce cognitive load requires further study.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {25},
numpages = {2},
keywords = {visualization, multiple representations theory, code tracing, introductory programming},
location = {},
series = {Koli Calling '24}
}

@article{11Kumar24,
author = {Harsh Kumar and Ilya Musabirov and Mohi Reza and Jiakai Shi and Xinyuan Wang and Joseph Jay Williams and Anastasia Kuzminykh and Michael Liut},
title = {Guiding Students in Using LLMs in Supported Learning Environments: Effects on Interaction Dynamics, Learner Performance, Confidence, and Trust},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3687038},
doi = {10.1145/3687038},
abstract = {Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role that teachers can play in shaping LLM-supported learning environments.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {499},
numpages = {30},
keywords = {artificial intelligence in education, collaborative learning with ai, human-ai collaboration, large language models, transparency, tutoring systems}
}

@article{10Bo24,
title = {Disclosures &amp; Disclaimers: Investigating the Impact of Transparency Disclosures and Reliability Disclaimers on Learner-LLM Interactions},
volume = {12},
url = {https://ojs.aaai.org/index.php/HCOMP/article/view/31597},
doi = {10.1609/hcomp.v12i1.31597},
abstract = {Large Language Models (LLMs) are increasingly being used in educational settings to assist students with assignments and learning new concepts. For LLMs to be effective learning aids, students must develop an appropriate level of trust and reliance on these tools. Misaligned trust and reliance can lead to suboptimal learning outcomes and reduced LLM engagement. Despite their growing presence, there is a limited understanding of achieving optimal transparency and reliance calibration in the educational use of LLMs. In a 3x2 between-subjects experiment conducted in a university classroom setting, we tested the effect of two transparency disclosures (System Prompt and Goal Summary) and an in-conversation Reliability Disclaimer on a GPT-4-based chatbot tutor provided to students for an assignment. Our findings suggest that disclaimer messages included in the responses may effectively mitigate learners’ overreliance on the LLM Tutor in the presence of incorrect advice. Disclosing System Prompt seemed to calibrate students’ confidence in their answers and reduce the occurrence of copy-pasting the exact assignment question to the LLM tutor. Student feedback indicated that they would like transparency framed in terms of performance-based metrics. Our work provides empirical insights on the design of transparency and reliability mechanisms for using LLMs in classrooms.},
number = {1},
journal = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
author = {Jessica Y. Bo and Harsh Kumar and Michael Liut and Ashton Anderson},
year = {2024},
month = {Oct.},
pages = {23-32}
}

inproceedings{08Sibia24,
author = {Naaz Sibia and Angela Zavaleta Bernuy and Tiana V. Simovic and Chloe Huang and Yinyue Tan and Eunchae Seong and Carolina Nobre and Daniel Zingaro and Michael Liut and Andrew Petersen},
title = {Exploring the Effects of Grouping by Programming Experience in Q&A Forums},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671107},
doi = {10.1145/3632620.3671107},
abstract = {Motivation: Q&A forums are a critical resource for supporting students in large educational environments, yet students often perceive these forums as stressful and report discomfort in participating visibly, especially in classes that are large and have students with varying levels of prior programming experience (PE). Method: We divided students in a CS1 Q&A forum into smaller, homogenous groups based on their PE. We use a mixed-methods approach to compare data from this experience to data from a setting where all students shared a single, large Q&A forum (a “mixed” setting). We quantitatively analyze measures of student engagement and use an open-ended qualitative approach to examine responses about student experience on the forums. This approach helps us identify the motivation behind student decisions to participate in visible or non-visible ways and to evaluate their alignment with theoretical frameworks. Results: In the mixed setting, students frequently use anonymity, with students without PE using anonymity more than students with PE and women using anonymity more than men. In contrast, in the homogenous groups, novices used anonymity less than novices in the mixed setting, while the students in higher-experience groups tended to use it more. We also observe a reduced anonymity usage among women in the homogenous experience groups, suggesting that PE plays a critical role in the observed gender disparities in forum participation. The qualitative analysis provides additional evidence that social status issues and confidence may explain these behavioral patterns. Conclusion: This study highlights the potential benefits and consequences of grouping students by experience. Homogenous PE groups foster increased student comfort and engagement within the Q&A forum for students with less experience, but students with more experience are exposed to more perceived status threats. We discuss how these results align with the theories we used to design the homogenous group setting. This exploration contributes to a deeper understanding of the underlying dynamics shaping student behavior in online learning communities. Educators and platform designers can use these lessons to more effectively create inclusive environments that accommodate diverse student needs and preferences.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {206–221},
numpages = {16},
keywords = {Anonymity, Confidence, Prior Experience, Q&A Forums},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{07Kumar24,
author = {Harsh Kumar and Ruiwei Xiao and Benjamin Lawson and Ilya Musabirov and Jiakai Shi and Xinyuan Wang and Huayin Luo and Joseph Jay Williams and Anna N. Rafferty and John Stamper and Michael Liut},
title = {Supporting Self-Reflection at Scale with Large Language Models: Insights from Randomized Field Experiments in Classrooms},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662042},
doi = {10.1145/3657604.3662042},
abstract = {Self-reflection on learning experiences constitutes a fundamental cognitive process, essential for consolidating knowledge and enhancing learning efficacy. However, traditional methods to facilitate reflection often face challenges in personalization, immediacy of feedback, engagement, and scalability. Integration of Large Language Models (LLMs) into the reflection process could mitigate these limitations. In this paper, we conducted two randomized field experiments in undergraduate computer science courses to investigate the potential of LLMs to help students engage in post-lesson reflection. In the first experiment (N=145), students completed a take-home assignment with the support of an LLM assistant; half of these students were then provided access to an LLM designed to facilitate self-reflection. The results indicated that the students assigned to LLM-guided reflection reported somewhat increased self-confidence compared to peers in a no-reflection control and a non-significant trend towards higher scores on a later assessment. Thematic analysis of students' interactions with the LLM showed that the LLM often affirmed the student's understanding, expanded on the student's reflection, and prompted additional reflection; these behaviors suggest ways LLM-interaction might facilitate reflection. In the second experiment (N=112), we evaluated the impact of LLM-guided self-reflection against other scalable reflection methods, such as questionnaire-based activities and review of key lecture slides, after assignment. Our findings suggest that the students in the questionnaire and LLM-based reflection groups performed equally well and better than those who were only exposed to lecture slides, according to their scores on a proctored exam two weeks later on the same subject matter. These results underscore the utility of LLM-guided reflection and questionnaire-based activities in improving learning outcomes. Our work highlights that focusing solely on the accuracy of LLMs can overlook their potential to enhance metacognitive skills through practices such as self-reflection. We discuss the implications of our research for the learning-at-scale community, highlighting the potential of LLMs to enhance learning experiences through personalized, engaging, and scalable reflection practices.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {86–97},
numpages = {12},
keywords = {field experiments, human-ai collaboration, large language models, learning engineering, self-reflection},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

inproceedings{07Liu24,
author = {Suqing Liu and Zezhu Yu and Feiran Huang and Yousef Bulbulia and Andreas Bergen and Michael Liut},
title = {Can Small Language Models With Retrieval-Augmented Generation Replace Large Language Models When Learning Computer Science?},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653554},
doi = {10.1145/3649217.3653554},
abstract = {Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system.To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s).We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {388–393},
numpages = {6},
keywords = {computing education, conversational agent, cs1, intelligence concentration, intelligent teaching assistant, intelligent tutoring system, large language models, locally deployable ai, personalized ai agent, retrieval augmented generation, small language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

inproceedings{06Zavaleta24,
author = {Angela Zavaleta Bernuy and Naaz Sibia and Pan Chen and Jessica Jia-Ni Xu and Elexandra Tran and Runlong Ye and Viktoria Pammer-Schindler and Andrew Petersen and Joseph Jay Williams and Michael Liut},
title = {Does the Medium Matter? An Exploration of Voice-Interaction for Self-Explanations},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661596},
doi = {10.1145/3643834.3661596},
abstract = {This research evaluates voice-based self-explanations as a pedagogical tool in preparation for lectures, assesses user preferences between voice and text, and derives design insights. We report two studies: Study 1, a quasi-experimental field study, with 247 participants divided into voice-based (N = 83), text-based (N = 81), and choice (N = 83) conditions. Study 2 uses semi-structured interviews (N = 16) to explore perceptions of the interaction paradigms in-depth. Results from the first study revealed a general preference for text, though voice users produced longer responses and more topic-related keywords. Over time, the preference for voice increased among students, from 10\% to 46\%, when given a choice. Study 2 suggested that factors like social presence contribute to hesitance toward voice-based explanations, with a cognitive load, self-confidence, and performance anxiety also influencing medium preferences. Our findings highlight design recommendations and demonstrate the potential of voice-based self-explanations in educational settings, indicating that mixed interfaces might better meet diverse needs.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {86–101},
numpages = {16},
keywords = {Active Learning, Explanation Prompts, Long-Term Memory, Self-Explanations, Student Performance, Text Explanations, Voice Explanations, Voice-based Interaction},
location = {IT University of Copenhagen, Denmark},
series = {DIS '24}
}

@INPROCEEDINGS{06Sheinman24,
  author = {Michael Sheinman Orenstrakh and Oscar Karnalim and Carlos Aníbal Suárez and Michael Liut},
  booktitle={2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Detecting LLM-Generated Text in Computing Education: Comparative Study for ChatGPT Cases}, 
  year={2024},
  volume={},
  number={},
  pages={121-126},
  url={https://doi.org/10.1109/COMPSAC61105.2024.00027},
  keywords={Measurement;Accuracy;Plagiarism;Large language models;Education;Detectors;Chatbots;Large Language Models;ChatGPT;GPT;AI Detectors;Plagiarism;Academic Integrity},
  doi={10.1109/COMPSAC61105.2024.00027}
}

inproceedings{04Sibia24,
author = {Naaz Sibia and Angela Zavaleta Bernuy and Elexandra Tran and Jessica Jia-Ni Xu and Joseph Jay Williams and Andrew Petersen and Michael Liut},
title = {Exploring Self-Explanations in a Flipped Database Course},
year = {2024},
isbn = {9798400706783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663649.3664374},
doi = {10.1145/3663649.3664374},
abstract = {Self-explanations show promise for engaging students with preparatory materials, yet research into the types of self-explanations submitted in computing is limited. This paper examines student perceptions of self-explanation prompts in a flipped databases course, building on existing research that highlights the advantages of self-explanations in such contexts. We present our findings on students’ perceptions of the utility of self-explanation prompts and analyze
the nature of the explanations generated across distinct topics. The results suggest that self-explanations not only facilitate a deeper understanding of the subject matter but also promote the discovery of new connections and examples through rewording explanations. Furthermore, errors within self-explanations offer valuable insights for the early identification of misconceptions.},
booktitle = {Proceedings of the 3rd International Workshop on Data Systems Education: Bridging Education Practice with Education Research},
pages = {20–26},
numpages = {7},
keywords = {Active Learning, Flipped Databases Course, Self-Explanations},
location = {Santiago, AA, Chile},
series = {DataEd '24}
}

@inproceedings{05Musabirov24,
author = {Ilya Musabirov and Angela Zavaleta Bernuy and Pan Chen and Michael Liut and Joseph Williams},
title = {Opportunities for Adaptive Experiments to Enable Continuous Improvement in Computer Science Education},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660659},
doi = {10.1145/3660650.3660659},
abstract = {Randomized A/B comparisons of alternative pedagogical strategies or other course improvements could provide useful empirical evidence for instructor decision-making. However, traditional experiments do not provide a straightforward pathway to rapidly utilize data, increasing the chances that students in an experiment experience the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might aid continuous course improvement. In adaptive experiments, data is analyzed and utilized as different conditions are deployed to students. This can be achieved using machine learning algorithms to identify which actions are more beneficial in improving students’ learning experiences and outcomes. These algorithms can then dynamically deploy the most effective conditions in subsequent interactions with students, resulting in better support for students’ needs. We illustrate this approach with a case study that provides a side-by-side comparison of traditional and adaptive experiments on adding self-explanation prompts in online homework problems in a CS1 course. This work paves the way for exploring the importance of adaptive experiments in bridging research and practice to achieve continuous improvement in educational settings.},
booktitle = {The 26th Western Canadian Conference on Computing Education},
articleno = {4},
numpages = {7},
keywords = {Thompson sampling, adaptive experimentation, continuous improvement, multiarmed bandit},
location = {<conf-loc>, <city>Kelowna</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {WCCCE '24}
}

@inproceedings{05Zavaleta24,
author = {Angela Zavaleta Bernuy and Andrew Chung and Alana Hodge and Ayesha Tayyiba and Michael Liut and Andrew Petersen},
title = {Student Transitions Through an Entire Computing Program},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660661},
doi = {10.1145/3660650.3660661},
abstract = {While the challenges experienced by first-year computing students have been well studied, little work has explored the transitions in disciplinary participation and challenges experienced by upper-years. This study explores how students’ needs and challenges evolve through a computing degree. We collected the experiences of first to final-year undergraduate computing students through surveys and interviews. We organized these experiences into themes that we compare against previous literature and illustrate with quotes. Upper-year students perceive changes in (a) levels of support and (b) the kinds of challenges they experience as they progress through the program. Second-year students feel pressured by the increasing difficulty of courses. This pressure increases through the third year as students begin to perceive a need to find employment. The experiences of our students suggest the need to better support the middle years of academic programs. Students in the first year are well-supported in their university transition, but students in the middle are often left to find their way as they develop a deeper understanding of their desired place in the field.},
booktitle = {The 26th Western Canadian Conference on Computing Education},
articleno = {5},
numpages = {7},
keywords = {CS1, student experience, transition, upper-year},
location = {<conf-loc>, <city>Kelowna</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {WCCCE '24}
}

@inproceedings{05Reza24,
author = {Mohi Reza and Nathan M Laundry and Ilya Musabirov and Peter Dushniku and Zhi Yuan "Michael" Yu and Kashish Mittal and Tovi Grossman and Michael Liut and Anastasia Kuzminykh and Joseph Jay Williams},
title = {ABScribe: Rapid Exploration \& Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641899},
doi = {10.1145/3613904.3641899},
abstract = {Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art Large Language Models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new variations without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration and organization of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly modify variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text fields for rapid in-place comparisons using mouse-over interactions on a popup toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions of the revision process (d = 2.41, p < 0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {1042},
numpages = {18},
keywords = {datasets, gaze detection, neural networks, text tagging},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{05Liut24,
author = {Ananya Bhattacharjee and Yuchen Zeng and Sarah Yi Xu and Dana Kulzhabayeva and Minyi Ma and Rachel Kornfield and Syed Ishtiaque Ahmed and Alex Mariakakis and Mary P Czerwinski and Anastasia Kuzminykh and Michael Liut and Joseph Jay Williams},
title = {Understanding the Role of Large Language Models in Personalizing and Scaffolding Strategies to Combat Academic Procrastination},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642081},
doi = {10.1145/3613904.3642081},
abstract = {Traditional interventions for academic procrastination often fail to capture the nuanced, individual-specific factors that underlie them. Large language models (LLMs) hold immense potential for addressing this gap by permitting open-ended inputs, including the ability to customize interventions to individuals' unique needs. However, user expectations and potential limitations of LLMs in this context remain underexplored. To address this, we conducted interviews and focus group discussions with 15 university students and 6 experts, during which a technology probe for generating personalized advice for managing procrastination was presented. Our results highlight the necessity for LLMs to provide structured, deadline-oriented steps and enhanced user support mechanisms. Additionally, our results surface the need for an adaptive approach to questioning based on factors like busyness. These findings offer crucial design implications for the development of LLM-based tools for managing procrastination while cautioning the use of LLMs for therapeutic guidance.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {15},
numpages = {18},
keywords = {ChatGPT, Education, GPT-4, Large Language Models, Personalized Reflections, Procrastination},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{03McGill24,
author = {Monica M. McGill and Sarah Heckman and Michael Liut and Ismaila Temitayo Sanusi and Claudia Szabo},
title = {Unlocking Excellence in Educational Research: Guidelines for High-Quality Research that Promotes Learning for All},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633402},
doi = {10.1145/3626253.3633402},
abstract = {While there are multiple standards bodies that define characteristics of high-quality, there are limited guidelines on conducting equity-enabling research, particularly in the context of high quality and in computing education. As part of an ACM ITiCSE Working Group in 2023, we engaged in a concept analysis and structured literature review to identify high-impact practices for conducting both high-quality and equity-enabling education research. As a result of this work, we produced a set of guidelines across each major phase of research that integrates characteristics of high-quality education research with those that are necessary for producing research that is designed to honor and meet the needs of various subgroups of learners. Special emphasis is given to the role that the researcher plays in shaping the research based upon how the researcher's lived experiences, perspectives, and training influences their work. During this special session, we will review each set of guidelines and engage attendees in reflection and discussion of them and how they can use the guidelines to enhance their education research.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1871–1872},
numpages = {2},
keywords = {computing education, cs for all, equity, guidelines, high-quality, post-secondary, primary, research, secondary},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{03Bui24,
author = {Giang Bui and Nicholas Susanto and Naaz Sibia and Angela Zavaleta Bernuy and Michael Liut and Andrew Petersen},
title = {Do Hints Enhance Learning in Programming Exercises? Exploring Students' Problem-Solving and Interactions},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635563},
doi = {10.1145/3626253.3635563},
abstract = {Asking for help (help-seeking) is a recognized and effective problem-solving strategy. This study investigates students' interaction with on-demand hints (automated hints requested by students) and assesses their impact on learning progress. We conducted an A/B experiment in a third-year computer science database course, offering hints for selected SQL problems with different hint designs. We collected data on students' code submissions, grades, and hint requests, and we administered a survey to gather feedback and gauge student perception of the hints. Many students accessed hints immediately without attempting the problem first, often requesting multiple hints in quick succession. While students perceived the hints to be valuable, we did not detect an impact on student problem-solving. These insights could inform future studies on the possible impact of students' attitudes toward hints, and how different types of hints might impact uptake and perception of hints.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1578–1579},
numpages = {2},
keywords = {help-seeking behavior, intelligent tutoring systems, on-demand hints, self-regulated learning},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{03Liut24,
author = {Michael Liut and Anna Ly and Jessica Jia-Ni Xu and Justice Banson and Paul Vrbik and Caroline D. Hardin},
title = {"I Didn't Know": Examining Student Understanding of Academic Dishonesty in Computer Science},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630753},
doi = {10.1145/3626252.3630753},
abstract = {In contrast with studies that have identified why students commit academic offences, many educators are familiar with the excuse that an accused student did not know the behavior counted as dishonest. Given the variations in policy and the ways collaboration and code sharing occur in professional and hobbyist spaces, this might be plausible. Mismatches between students' conceptions of academic honesty and course policy can have major consequences, from being kicked out of programs to being too nervous to study with peers. In this work, we investigate what students understand about academic integrity in computer science courses and if there are differences based on university, country, demographic, or online versus in-person courses. We present a study that surveys undergraduate computer science students (N = 1,011) at three universities (Australia, Canada, and the United States of America). The results show that all three institutions take academic integrity seriously, and their students are aware of its importance, but confusion on what is covered under the policies is common. Interestingly, the results also show that course instructors play a huge role as to what students perceive to be a violation of the academic integrity policy at their institution. By understanding student's perspectives on academic integrity, educators can better develop policies and practices that reduce inadvertent and mistaken violations of academic integrity policies.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {757–763},
numpages = {7},
keywords = {academic dishonesty, academic integrity, assessment, cheating, computer science students, computing students, education},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

inproceedings{03Sibia24,
author = {Naaz Sibia and Giang Bui and Bingcheng Wang and Yinyue Tan and Angela Zavaleta Bernuy and Christina Bauer and Joseph Jay Williams and Michael Liut and Andrew Petersen},
title = {Examining Intention to Major in Computer Science: Perceived Potential and Challenges},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630843},
doi = {10.1145/3626252.3630843},
abstract = {This study explores links between attributes of computing students, such as prior programming experience (PE) and gender, with expectations for success and the perception of challenges. Using Expectancy-Value Theory (EVT), we investigate their major intentions and the impact of these factors post-CS1. Data was gathered using surveys at the beginning and end of an introductory programming course, focusing on demographics, expectations of success, and perceptions of challenges. Application status for the computing major was also recorded. Our results revealed that men and students with PE generally perceived greater potential for success and reported facing fewer challenges. In contrast, women and students without PE more often indicated concerns about intellectual ability and perceived challenges less positively. Notably, while gender appears in the preceding results, an intersectional analysis indicates that PE is the central factor. PE is also linked to persistence in the field of computing. Our results further highlight the importance of providing students with opportunities to develop experience, as it can help shape their expectations, perceived challenges, and retention in computing.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1237–1243},
numpages = {7},
keywords = {gender, prior experience, program retention, self-efficacy},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@inproceedings{01Miedema23,
author = {Daphne Miedema and Michael Liut and George H. L. Fletcher and Efthimia Aivaloglou},
title = {“There is no ambiguity on what to return”: Investigating the Prevalence of SQL Misconceptions},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631821},
doi = {10.1145/3631802.3631821},
abstract = {In recent years, database education has been receiving more attention, with research in various directions such as the development of tools for education, the analysis of students’ homework, and the exploration of misconceptions. Misconceptions are mistakes in student reasoning that lead to errors during problem-solving. Recent work has documented misconceptions and errors in SQL. In this study we test the prevalence of several of these misconceptions through a multiple-choice questionnaire, to see if they hold on a larger, more diverse, student population. We found that all misconceptions are held to some extent, with prevalence scores ranging from one to fifty-two percent of the student population. Additionally, we have uncovered previously unidentified areas of struggle, allowing us to identify new misconceptions.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {10},
numpages = {12},
keywords = {Data Systems Education, Databases, Misconceptions, SQL},
location = {<conf-loc>, <city>Koli</city>, <country>Finland</country>, </conf-loc>},
series = {Koli Calling '23}
}

inproceedings{11Zavaleta23,
author = {Angela Zavaleta Bernuy and Runlong Ye and Elexandra Tran and Naaz Sibia and Abhijoy Mandal and Hammad Shaikh and Bogdan Simion and Michael Liut and Andrew Petersen and Joseph Jay Williams},
title = {Do Students Read Instructor Emails? A Case Study of Intervention Email Open Rates},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631813},
doi = {10.1145/3631802.3631813},
abstract = {Email is an important mode of communication because it scales to the largest computing courses and is institutionally supported. Furthermore, regular email communication from instructors has been shown to help set student expectations and encourage participation. As a result, effective email can contribute to emotional engagement, which has been linked to improvements in performance and retention, the latter being a persistent problem in computer science. However, we lack a clear picture of how computing students interact with emails and whether their use aligns with instructors’ expectations. This paper addresses this gap by presenting data on how often CS1 students open instructor emails. We present email engagement data throughout the term for a particular type of email that prompts students to plan to start their homework. Contrary to instructors’ expectations, the rate at which students open emails of this kind does not change significantly over the term. Many students who engage with the emails do so consistently, even after repeated emails throughout the term. The patterns we found illustrate the value of collecting this type of data and informing instructors and researchers about who reads these messages and how often they actually reach students.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {13},
numpages = {12},
keywords = {email engagement, email open rates, instructor communication},
location = {<conf-loc>, <city>Koli</city>, <country>Finland</country>, </conf-loc>},
series = {Koli Calling '23}
}

@inproceedings{12McGill23,
author = {Monica M. McGill and Sarah Heckman and Christos Chytas and Michael Liut and Vera Kazakova and Ismaila Temitayo Sanusi and Selina Marianna Shah and Claudia Szabo},
title = {Conducting Sound, Equity-Enabling Computing Education Research},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633495},
doi = {10.1145/3623762.3633495},
abstract = {Problem. To investigate and identify promising practices in equitable K-12 and tertiary computer science (CS) education, the capacity for education researchers to conduct this research must be rapidly built globally. Simultaneously, concerns have arisen over the last few years about the quality of research that is being conducted and the lack of research that supports teaching all students computing.Research Question. Our research question for our study was: In what ways can existing research standards and practices inform methodologically sound, equity-enabling computing education research?Methodology. We conducted a concept analysis using existing research and various standards (e.g. European Educational Research Association, Australian Education Research Organisation, American Psychological Association). We then synthesised key features in the context of equity-focused K-12 computing education research.Findings. We present a set of guidelines for general research design that takes into account best practices across the standards that are infused with equity-enabling research practices.Implications. Our guidelines will directly impact future equitable computing education research by providing guidance on conducting high-quality research such that the findings can be aggregated and impact future policy with evidence-based results. Because we have crafted these guidelines to be broadly applicable across a variety of settings, we believe that they will be useful to researchers operating in a variety of contexts.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {30–56},
numpages = {27},
keywords = {computer science education research, computing education, equity, evidence, high quality, k-12, post-secondary, primary, research, secondary, standards, tertiary},
location = {Turku, Finland},
series = {ITiCSE-WGR '23}
}

@inproceedings{09Miedema23,
author = {Daphne Miedema and Michael Liut and George Fletcher and Efthimia Aivaloglou},
title = {MSMI1: Towards a Validated SQL Misconceptions Instrument},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603471},
doi = {10.1145/3568812.3603471},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {16–17},
numpages = {2},
location = {Chicago, IL, USA},
series = {ICER '23}
}

inproceedings{07Bernuy23,
author = {Angela Zavaleta Bernuy and Anna Ly and Brian Harrington and Michael Liut and Sadia Sharmin and Lisa Zhang and Andrew Petersen},
title = {"I Am Not Enough": Impostor Phenomenon Experiences of University Students},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588779},
doi = {10.1145/3587102.3588779},
abstract = {Recent work has confirmed that computing students experience the Imposter Phenomenon (IP) at higher rates than reported in other disciplines. However, no work has examined what aspects of the university computing experience might lead to a higher rate of IP experiences. We aim to illustrate the IP experiences students have, identify common sources of these experiences, and document the effects of these experiences and how students respond to them. We asked undergraduate students to share recent experiences that illustrate their experiences with the IP. We conducted an inductive thematic analysis on these open-ended responses, resulting in a set of inter-connected themes. A significant fraction of students related stories about making comparisons with peers or observing peer behaviour that made them question their abilities. Students also spoke about holding unrealistic expectations learned from their peers or imposed by the environment. These experiences may be particularly acute for minority-affiliated students who may come to feel they do not belong. Ultimately, these IP experiences can lead to a loss of motivation or a cycle of failure that leads students to leave computing. The central role social comparisons play in IP experiences suggests that it is particularly important to foster communities where opportunities for comparison are reduced and where realistic expectations are explicitly set.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {313–319},
numpages = {7},
keywords = {impostor phenomenon, qualitative, IP, survey},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

inproceedings{07Sibia23,
author = {Naaz Sibia and Angela Zavaleta Bernuy and Joseph Jay Williams and Michael Liut and Andrew Petersen},
title = {Student Usage of Q\&A Forums: Signs of Discomfort?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588842},
doi = {10.1145/3587102.3588842},
abstract = {Q\&A forums are widely used in large classes to provide scalable support. In addition to offering students a space to ask questions, these forums aim to create a community and promote engagement. Prior literature suggests that the way students participate in Q\&A forums varies and that most students do not actively post questions or engage in discussions. Students may display different participation behaviours depending on their comfort levels in the class. This paper investigates students' use of a Q\&A forum in a CS1 course. We also analyze student opinions about the forum to explain the observed behaviour, focusing on students' lack of visible participation (lurking, anonymity, private posting). We analyzed forum data collected in a CS1 course across two consecutive years and invited students to complete a survey about perspectives on their forum usage. Despite a small cohort of highly engaged students, we confirmed that most students do not actively read or post on the forum. We discuss students' reasons for the low level of engagement and barriers to participating visibly. Common reasons include fearing a lack of knowledge and repercussions from being visible to the student community.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {33–39},
numpages = {7},
keywords = {confidence, Q\&A forums, gender, anonymity},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{06Bernuy23,
author = {Angela Zavaleta Bernuy and Naaz Sibia and Pan Chen and Chloe Huang and Andrew Petersen and Joseph Jay Williams and Michael Liut},
title = {VoiceEx: Voice Submission System for Interventions in Education},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3595284},
doi = {10.1145/3587103.3595284},
abstract = {Generating self-explanations has been identified as a successful strategy in helping learners engage with course content and organize what they learn in a structured format. While typing an explanation may allow more structure and formality, explaining by voice can be more natural and help free cognitive resources to focus on learning goals and understanding concepts. As we investigated the effects and students' perceptions of using voice or text to self-explain new course concepts, we failed to find a tool that would meet our needs. We present our work in designing and developing VoiceEx, a submission courseware that allows text and voice input to collect data in both mediums. VoiceEx was created to support a self-explanations intervention for computer science students; however, given its features and the advantages of being able to collect spoken responses, it can be used in a variety of environments. Future refinement of this tool includes artificial intelligence features to better guide students' submissions.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {585–586},
numpages = {2},
keywords = {educational technology, voice recording, voice submission},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{06Zavaleta23,
author = {Angela Zavaleta Bernuy and Jessica Jia-Ni Xu and Naaz Sibia and Joseph Jay Williams and Andrew Petersen and Michael Liut},
title = {Self-Explanation Modality: Effects on Student Performance?},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594188},
doi = {10.1145/3587103.3594188},
abstract = {In this poster, we present a pilot study investigating the impact of the medium used for self-explanation on students' performance outcomes in a databases course. We did not see notable differences in student performance based on the medium they used for self-explanation. We also note that while most students prefer using text to submit self-explanations, their preferences may differ when they use voice.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {641},
numpages = {1},
keywords = {educational technology, reflection prompts, self-explanations, voice recording},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{04Reza23,
author = {Mohi Reza and Ilya Musabirov and Nathan Laundry and Michael Liut and Joseph Jay Williams},
title = {Using A/B Testing as a Pedagogical Tool for Iterative Design in HCI Classrooms},
year = {2023},
isbn = {9798400707377},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587399.3587412},
doi = {10.1145/3587399.3587412},
abstract = {This paper explores the use of A/B testing as a pedagogical tool for iterative design in HCI classrooms and outlines a vision for experiment-inspired design. The traditional focus on the statistical aspects of A/B testing education has meant that the equally crucial role of iterative design embedded within the experimental process has not received commensurate attention. By incorporating iterative design learning activities that are scaffolded by A/B testing tools, HCI students can gain transferable skills and experience in doing multiple cycles of ideation, prototyping, testing, and evaluation, and simultaneously contribute to continual course improvement. We reconsider the role of experimentation in HCI education as a means for exploring complex design spaces. Drawing from our experience in teaching this approach and conducting education research involving sequences of online controlled experiments, we present examples of how to use A/B testing as a pedagogical tool for iterative design in HCI classrooms.},
booktitle = {Proceedings of the 5th Annual Symposium on HCI Education},
pages = {43–48},
numpages = {6},
keywords = {A/B Testing, HCI Education, Iterative Design},
location = {Hamburg, Germany},
series = {EduCHI '23}
}

@inproceedings{03Tran23,
author = {Elexandra Tran and Angela Zavaleta Bernuy and Bogdan Simion and Michael Liut and Andrew Petersen and Joseph Jay Williams},
title = {Investigating Subject Lines Length on Students' Email Open Rates},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3576307},
doi = {10.1145/3545947.3576307},
abstract = {Instructors often prefer to use email for course communication. The use of emails has been widely discussed in the fields of marketing and behavioural design, but the prevalence of email in education makes it important for instructors to collect metrics on emails to see how students engage with them. One component of emails are the subject lines, which constitute as one of the first things a receiver sees before deciding to open an email. This poster discusses a case study at deploying an email intervention in an online CS1 course. We investigate how the length of subject lines impact the rate at which students open emails of a particular type that prompts them to start their homework early. We aim to share key results to inform instructors how to design their emails to better reach students. Further, we highlight the potential benefits for instructors when collecting and analyzing email engagement data.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1363},
numpages = {1},
keywords = {email, open rates, student engagement},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{03Musabirov23,
author = {Ilya Musabirov and Angela Zavaleta Bernuy and Michael Liut and Joseph Jay Williams},
title = {A Case Study in Opportunities for Adaptive Experiments to Enable Rapid Continuous Improvement},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3576225},
doi = {10.1145/3545947.3576225},
abstract = {Drawing inspiration from machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. We discuss an example side-by-side comparison of traditional and adaptive experimentation of self-explanation prompts in online homework problems in a CS1 course. This provides the first step in exploring the future of how this approach can help bridge research and practice in continuous course improvement.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1279},
numpages = {1},
keywords = {adaptive experiments, continuous improvement, field experiments, multiarmed bandits, thompson sampling},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{03Williams23,
author = {Joseph Jay Williams and Nathan Laundry and Ilya Musabirov and Angela Zavaleta Bernuy and Michael Liut},
title = {Designing, Deploying, and Analyzing Adaptive Educational Field Experiments},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3569635},
doi = {10.1145/3545947.3569635},
abstract = {Digital experiments can be used in CSedu to test hypotheses about interventions and conditions' efficacy (or inefficacy). This workshop will discuss and deconstruct the design process and analysis for various experiments conducted in CS1. E.g., experiments testing which explanations students find helpful, which emails get them to start homework early, or which webpages effectively encourage and motivate students. This workshop teaches participants how to conduct, interpret, and analyze adaptive field experiments. These adaptive experiments employ machine learning algorithms to analyze experiments during deployment and dynamically shift the allocation of arms/conditions to give future students better conditions more rapidly. Adaptive field experiments can accelerate scientific discovery by enabling more complex experimental designs and increasing statistical power by phasing conditions in and out more efficiently. The workshop is supported by a 5-year NSF grant to build software tools and a digital community, gathering instructors, domain scientists and methodologists to teach them how to run adaptive experiments. The methodological focus includes understanding: (1) which algorithms are best for adaptive experiments that meet domain scientists' needs in specific experimental designs and data sets; (2) which hypothesis tests and Bayesian analyses to choose. Software companies use these innovative methodologies extensively to continuously improve product design. This workshop demonstrates how the same methods can be used in CSedu to improve research rigor and accelerate educational research implementation, ultimately improving student outcomes.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1179},
numpages = {1},
keywords = {adaptive field experiments, computer science education, experimental design, machine learning, multiarmed bandits, statistical analysis},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{03Bui23,
author = {Giang Bui and Naaz Sibia and Angela Zavaleta Bernuy and Michael Liut and Andrew Petersen},
title = {Prior Programming Experience: A Persistent Performance Gap in CS1 and CS2},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569752},
doi = {10.1145/3545945.3569752},
abstract = {Previous work has reported on the advantageous effects of prior experience in CS1, but it remains unclear whether these effects fade over a sequence of introductory programming courses. Furthermore, while student perceptions suggest that prior experience remains important, studies have reported that a student's expectation of their performance is a more accurate predictor of outcome. We aim to confirm if prior experience (formal or informal) provides short-term and long-term advantages in computing courses or if the advantage fades. Furthermore, we explore whether the expectation of performance is a more accurate predictor of student success than informal and formal prior experience. To explore these questions, we deployed surveys in a CS1 course to gauge students' level of prior experience in programming, prediction of final exam grades, and self-efficacy to succeed in university. Grades from CS1 and CS2 were also collected. We observed a persistent (1-letter grade) gap between the performance of students with no prior experience and those with any experience, but we did not observe a noteworthy gap when comparing student performance based on formal or informal experience. We also observed differences in self-efficacy and retention rates between different levels of prior experience. Lastly, we confirm that success in CS1 can be better reflected and predicted by some controllable factors, such as students' perceptions of ability.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {889–895},
numpages = {7},
keywords = {cs2, prior experience, self-efficacy, confidence, cs1, prediction},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{06Liut22,
author = {Naaz Sibia and Michael Liut},
title = {The Positive Effects of Using Reflective Prompts in a Database Course},
year = {2022},
isbn = {9781450393508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531072.3535323},
doi = {10.1145/3531072.3535323},
abstract = {Motivation: Prior literature has identified student reflections as a way to encourage students to express their thoughts in a structured and focused manner. Objectives: Our goal is to examine the impact of reflections in a third year database systems course, which employs an active learning approach and classroom environment. Specifically, we are interested in seeing whether reflecting on key concepts covered in a preparatory component before lecture had an impact on student’s immediate and long-term performance. Methods: Students were divided into two groups, and asked to reflect on different topics after watching lecture videos before completing their homework exercises for 3 weeks. Results: We observed that students who reflected on lecture concepts performed better on homework exercises that covered those same concepts than students who did not reflect on those same concepts. Moreover, students who reflected performed better in subsequent assessments than students who did not reflect at all. Implications: Reflection as a part of the preparatory component in flipped classrooms is a useful component in conceptual understanding. Further research and investigation should be pursued into ways of prompting reflection, and assessing this component in database courses.},
booktitle = {1st International Workshop on Data Systems Education},
pages = {32–37},
numpages = {6},
keywords = {Active Learning, Long-Term Memory, Computer Science Education, Reflective Prompts, Databases, Student Performance},
location = {Philadelphia, PA, USA},
series = {DataEd '22}
}

inproceedings{03Harrington22,
author = {Angela Zavaleta Bernuy and Anna Ly and Brian Harrington and Michael Liut and Andrew Petersen and Sadia Sharmin and Lisa Zhang},
title = {Additional Evidence for the Prevalence of the Impostor Phenomenon in Computing},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499282},
doi = {10.1145/3478431.3499282},
abstract = {Motivation Despite the widespread belief that computing practitioners frequently experience the Imposter Phenomenon (IP), little formal work has measured the prevalence of IP in the computing community despite its negative effect on achievement.Objectives This study aims to replicate recent work that has suggested that IP experiences are widespread in computing students and to extend that work by exploring the relationship between the IP, progress in the program, and ethnic identity.Methods A survey with several demographic questions (gender, ethnicity, international status, and year of study) and Clance's IP scale (CIPS) was deployed to students in post-secondary computing courses. Correlations between demographic factors and CIPS scores were evaluated, and a linear model was constructed to explore the interaction between demographic factors of interest.Results We reaffirm that a high proportion of CS students meet the IP diagnostic criteria and that women report higher CIPS scores than men. We also present evidence that Asian students with domestic and international status report different levels of IP experiences.Discussion These findings highlight the importance – to educators at all levels – of cultivating belonging in computing communities.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education},
pages = {654–660},
numpages = {7},
keywords = {imposter syndrome, impostor phenomenon, belonging},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{03Liut22,
author = {Pan Chen and Naaz Sibia and Angela Zavaleta Bernuy and Michael Liut and Joseph Jay Williams},
title = {Investigating the Impact of Voice Response Options in Surveys},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478432.3499087},
doi = {10.1145/3478432.3499087},
abstract = {With the widespread usage of mobile devices, users can now choose to provide input through voice or text. As researchers frequently ask students open-ended questions, we want to explore a natural mode to obtain better feedback in surveys. This study details a preliminary study demonstrating the importance of allowing students to choose between voice or text input to respond to surveys. A survey with several open-ended questions was deployed in a CS1 course. Correlations between the gender of the respondent and their method of responding were evaluated. We found that voice responses tended to be longer and preferred more by females relative to male students.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1124},
numpages = {1},
keywords = {surveys, open-ended questions, voice responses, cs1},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{11Liut20,
author = {Muyu Wang and Naaz Sibia and Ilir Dema and Michael Liut and Carlos Aníbal Suárez},
title = {Building a Better SQL Automarker for Database Courses},
year = {2021},
isbn = {9781450384889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488042.3489970},
doi = {10.1145/3488042.3489970},
abstract = {This work introduces and demonstrates the viability of a novel SQL automarking tool (“SQAM”) that: (1) provides a fair grade to the student, one which matches the student’s effort and understanding of the course material, and (2) to provide personalized feedback, allowing the student to remain engaged in the material and learn from their mistakes while still being in that headspace. Additionally, we strive to ensure that our tool maintains the same standards (grade and feedback) that a highly qualified member of teaching staff would produce, so we compare and contrast our automarker’s results to that of teaching assistants over several historic offerings of the same database course at a large research intensive public institution, while reducing the grading time, thus enabling the teaching staff to channel more time into instruction. Furthermore, we describe SQAM’s design and our model which applies the aggregate result of four different string similarity metrics to compute solution similarity in conjunction with our discretization process to fairly evaluate a student’s submission. Our results show that SQAM produces very similar grades to those which were historically given by teaching assistants. },
booktitle = {21st Koli Calling International Conference on Computing Education Research},
articleno = {37},
numpages = {3},
keywords = {SQL Automarking, Partial Marking, String Regularities, String Similarity, Database Course Tools, Student Feedback Enhancement},
location = {Joensuu, Finland},
series = {Koli Calling '21}
}

@inproceedings{11ly21,
author = {Anna Ly and Jack Parkinson and Quintin Cutts and Michael Liut and Andrew Petersen},
title = {Spatial Skills and Demographic Factors in CS1},
year = {2021},
isbn = {9781450384889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488042.3488049},
doi = {10.1145/3488042.3488049},
abstract = {Motivation Prior studies have established that training spatial skills may improve outcomes in computing courses. Very few of these studies have, however, explored the impact of spatial skills training on women or examined its relationship with other factors commonly explored in the context of academic performance, such as socioeconomic background and self-efficacy. Objectives In this study, we report on a spatial skills intervention deployed in a computer programming course (CS1) in the first year of a post-secondary program. We explore the relationship between various demographic factors, course performance, and spatial skills ability at both the beginning and end of the term. Methods Data was collected using a combination of demographic surveys, existing self-efficacy and CS1 content instruments, and the Revised PVST:R spatial skills assessment. Spatial skills were evaluated both at the beginning of the term and at the end, after spatial skills training was provided. Results While little evidence was found to link spatial skills to socioeconomic status or self-efficacy, both gender identity and previous experience in computing were found to be correlated to spatial skills ability at the start of the course. Women initially recorded lower spatial skills ability, but after training, the distribution of spatial skills scores for women approached that of men. Discussion These findings suggest that, if offered early enough, spatial skills training may be able to remedy some differences in background that impact performance in computing courses. },
booktitle = {21st Koli Calling International Conference on Computing Education Research},
articleno = {4},
numpages = {10},
keywords = {retention, spatial skills, socioeconomic status, CS1, gender},
location = {Joensuu, Finland},
series = {Koli Calling '21}
}

@article{12parkinson21,
author = {Jack Parkinson and Ryan Bockmon and Quintin Cutts and Michael Liut and Andrew Petersen and Sheryl Sorby},
title = {Practice Report: Six Studies of Spatial Skills Training in Introductory Computer Science},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3494574},
doi = {10.1145/3494574},
journal = {ACM Inroads},
month = {nov},
pages = {18–29},
numpages = {12}
}

@inproceedings{10ly2021revisiting,
  title={Revisiting Syntax Exercises in CS1},
  author = {Anna Ly and John Edwards and Michael Liut and Andrew Petersen},
  booktitle={Proceedings of the 22st Annual Conference on Information Technology Education},
  pages={9--14},
  year={2021},
  url = {https://doi.org/10.1145/3450329.3476855},
  doi = {10.1145/3450329.3476855},
}

inproceedings{03Zhang21,
author = {Larry Yueli Zhang and Andrew K. Petersen and Michael Liut and Bogdan Simion and Furkan Alaca},
title = {A Multi-Course Report on the Experience of Unplanned Online Exams},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432515},
doi = {10.1145/3408877.3432515},
abstract = {We report our experience of preparing and conducting unplanned online exams in the unique half-physical, half-virtual semester of Winter 2020. The report covers four courses in a large university's computer science program, ranging from first-year to third-year. With the data generated by students taking both in-person and online exams in multiple courses, we perform analyses to evaluate the validity of the online exams (especially the unproctored ones) as an assessment of student understanding. With the fine-grained student activity data provided by the online exam platform, we are also able to investigate the patterns in student exam-taking behaviours and their correlations with student performance on the exam. In addition, we share, in detail, the tips and lessons that were learned throughout the process of designing, implementing, and hosting the online exams.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {17–23},
numpages = {7},
keywords = {student behaviour, online exams, assessment},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{08Liut20,
 author = {Arnaud Deza and Haocheng Hu and Vaishvik Maisuria and Michael Liut and Andrew Petersen and Bogdan Simion},
 title = {Using Discussion Board Data to Hire Teaching Assistants},
 booktitle = {Proceedings of the 6th SPLICE Workshop at L@S},
 year = {2020},
 numpage = {6},
 url = {https://cssplice.github.io/LAS20/proc/SPLICE_2020_LS_paper_9.pdf},
}

@inproceedings{06Liut20,
author = {Simon and Oscar Karnalim and Judy Sheard and Ilir Dema and Amey Karkare and Juho Leinonen and Michael Liut and Renee McCauley},
title = {Choosing Code Segments to Exclude from Code Similarity Detection},
year = {2020},
isbn = {9781450382939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437800.3439201},
doi = {10.1145/3437800.3439201},
booktitle = {Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {1–19},
numpages = {19},
keywords = {collusion, academic integrity, plagiarism, code similarity detection},
location = {Trondheim, Norway},
series = {ITiCSE-WGR '20}
}
