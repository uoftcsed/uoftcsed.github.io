@inproceedings{01Miedema25,
author = {Daphne Miedema and Toni Taipalus and Vangel V. Ajanovski and Abdussalam Alawini and Martin Goodfellow and Michael Liut and Svetlana Peltsverger and Tiffany Young},
title = {Data Systems Education: Curriculum Recommendations, Course Syllabi, and Industry Needs},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709609},
doi = {10.1145/3689187.3709609},
abstract = {Data systems have been an important part of computing curricula for decades, and an integral part of data-focused industry roles such as software developers, data engineers, and data scientists. However, the field of data systems encompasses a large number of topics ranging from data manipulation and database distribution to creating data pipelines and data analytics solutions. Due to the slow nature of curriculum development, it remains unclear (i) which data systems topics are recommended across diverse higher education curriculum guidelines, (ii) which topics are taught in higher education data systems courses, and (iii) which data systems topics are actually valued in data-focused industry roles. In this study, we analyzed computing curriculum guidelines, course contents, and industry needs regarding data systems to uncover discrepancies between them. Our results show, for example, that topics such as data visualization, data warehousing, and semi-structured data models are valued in industry, yet seldom taught in courses. This work allows professionals to further align curriculum guidelines, higher education, and data systems industry to better prepare students for their working life by focusing on relevant skills in data systems education.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {95–123},
numpages = {29},
keywords = {curriculum, data engineering, data systems, database, education, industry, knowledge gap, skill set},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{11Kumar24,
author = {Harsh Kumar and Ilya Musabirov and Mohi Reza and Jiakai Shi and Xinyuan Wang and Joseph Jay Williams and Anastasia Kuzminykh and Michael Liut},
title = {Guiding Students in Using LLMs in Supported Learning Environments: Effects on Interaction Dynamics, Learner Performance, Confidence, and Trust},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3687038},
doi = {10.1145/3687038},
abstract = {Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role that teachers can play in shaping LLM-supported learning environments.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {499},
numpages = {30},
keywords = {artificial intelligence in education, collaborative learning with ai, human-ai collaboration, large language models, transparency, tutoring systems}
}

@article{10Bo24,
title = {Disclosures &amp; Disclaimers: Investigating the Impact of Transparency Disclosures and Reliability Disclaimers on Learner-LLM Interactions},
volume = {12},
url = {https://ojs.aaai.org/index.php/HCOMP/article/view/31597},
doi = {10.1609/hcomp.v12i1.31597},
abstract = {Large Language Models (LLMs) are increasingly being used in educational settings to assist students with assignments and learning new concepts. For LLMs to be effective learning aids, students must develop an appropriate level of trust and reliance on these tools. Misaligned trust and reliance can lead to suboptimal learning outcomes and reduced LLM engagement. Despite their growing presence, there is a limited understanding of achieving optimal transparency and reliance calibration in the educational use of LLMs. In a 3x2 between-subjects experiment conducted in a university classroom setting, we tested the effect of two transparency disclosures (System Prompt and Goal Summary) and an in-conversation Reliability Disclaimer on a GPT-4-based chatbot tutor provided to students for an assignment. Our findings suggest that disclaimer messages included in the responses may effectively mitigate learners’ overreliance on the LLM Tutor in the presence of incorrect advice. Disclosing System Prompt seemed to calibrate students’ confidence in their answers and reduce the occurrence of copy-pasting the exact assignment question to the LLM tutor. Student feedback indicated that they would like transparency framed in terms of performance-based metrics. Our work provides empirical insights on the design of transparency and reliability mechanisms for using LLMs in classrooms.},
number = {1},
journal = {Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
author = {Jessica Y. Bo and Harsh Kumar and Michael Liut and Ashton Anderson},
year = {2024},
month = {Oct.},
pages = {23-32}
}

inproceedings{08Sibia24,
author = {Naaz Sibia and Angela Zavaleta Bernuy and Tiana V. Simovic and Chloe Huang and Yinyue Tan and Eunchae Seong and Carolina Nobre and Daniel Zingaro and Michael Liut and Andrew Petersen},
title = {Exploring the Effects of Grouping by Programming Experience in Q&A Forums},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671107},
doi = {10.1145/3632620.3671107},
abstract = {Motivation: Q&A forums are a critical resource for supporting students in large educational environments, yet students often perceive these forums as stressful and report discomfort in participating visibly, especially in classes that are large and have students with varying levels of prior programming experience (PE). Method: We divided students in a CS1 Q&A forum into smaller, homogenous groups based on their PE. We use a mixed-methods approach to compare data from this experience to data from a setting where all students shared a single, large Q&A forum (a “mixed” setting). We quantitatively analyze measures of student engagement and use an open-ended qualitative approach to examine responses about student experience on the forums. This approach helps us identify the motivation behind student decisions to participate in visible or non-visible ways and to evaluate their alignment with theoretical frameworks. Results: In the mixed setting, students frequently use anonymity, with students without PE using anonymity more than students with PE and women using anonymity more than men. In contrast, in the homogenous groups, novices used anonymity less than novices in the mixed setting, while the students in higher-experience groups tended to use it more. We also observe a reduced anonymity usage among women in the homogenous experience groups, suggesting that PE plays a critical role in the observed gender disparities in forum participation. The qualitative analysis provides additional evidence that social status issues and confidence may explain these behavioral patterns. Conclusion: This study highlights the potential benefits and consequences of grouping students by experience. Homogenous PE groups foster increased student comfort and engagement within the Q&A forum for students with less experience, but students with more experience are exposed to more perceived status threats. We discuss how these results align with the theories we used to design the homogenous group setting. This exploration contributes to a deeper understanding of the underlying dynamics shaping student behavior in online learning communities. Educators and platform designers can use these lessons to more effectively create inclusive environments that accommodate diverse student needs and preferences.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {206–221},
numpages = {16},
keywords = {Anonymity, Confidence, Prior Experience, Q&A Forums},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{07Kumar24,
author = {Harsh Kumar and Ruiwei Xiao and Benjamin Lawson and Ilya Musabirov and Jiakai Shi and Xinyuan Wang and Huayin Luo and Joseph Jay Williams and Anna N. Rafferty and John Stamper and Michael Liut},
title = {Supporting Self-Reflection at Scale with Large Language Models: Insights from Randomized Field Experiments in Classrooms},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662042},
doi = {10.1145/3657604.3662042},
abstract = {Self-reflection on learning experiences constitutes a fundamental cognitive process, essential for consolidating knowledge and enhancing learning efficacy. However, traditional methods to facilitate reflection often face challenges in personalization, immediacy of feedback, engagement, and scalability. Integration of Large Language Models (LLMs) into the reflection process could mitigate these limitations. In this paper, we conducted two randomized field experiments in undergraduate computer science courses to investigate the potential of LLMs to help students engage in post-lesson reflection. In the first experiment (N=145), students completed a take-home assignment with the support of an LLM assistant; half of these students were then provided access to an LLM designed to facilitate self-reflection. The results indicated that the students assigned to LLM-guided reflection reported somewhat increased self-confidence compared to peers in a no-reflection control and a non-significant trend towards higher scores on a later assessment. Thematic analysis of students' interactions with the LLM showed that the LLM often affirmed the student's understanding, expanded on the student's reflection, and prompted additional reflection; these behaviors suggest ways LLM-interaction might facilitate reflection. In the second experiment (N=112), we evaluated the impact of LLM-guided self-reflection against other scalable reflection methods, such as questionnaire-based activities and review of key lecture slides, after assignment. Our findings suggest that the students in the questionnaire and LLM-based reflection groups performed equally well and better than those who were only exposed to lecture slides, according to their scores on a proctored exam two weeks later on the same subject matter. These results underscore the utility of LLM-guided reflection and questionnaire-based activities in improving learning outcomes. Our work highlights that focusing solely on the accuracy of LLMs can overlook their potential to enhance metacognitive skills through practices such as self-reflection. We discuss the implications of our research for the learning-at-scale community, highlighting the potential of LLMs to enhance learning experiences through personalized, engaging, and scalable reflection practices.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {86–97},
numpages = {12},
keywords = {field experiments, human-ai collaboration, large language models, learning engineering, self-reflection},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

inproceedings{07Liu24,
author = {Suqing Liu and Zezhu Yu and Feiran Huang and Yousef Bulbulia and Andreas Bergen and Michael Liut},
title = {Can Small Language Models With Retrieval-Augmented Generation Replace Large Language Models When Learning Computer Science?},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653554},
doi = {10.1145/3649217.3653554},
abstract = {Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system.To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s).We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {388–393},
numpages = {6},
keywords = {computing education, conversational agent, cs1, intelligence concentration, intelligent teaching assistant, intelligent tutoring system, large language models, locally deployable ai, personalized ai agent, retrieval augmented generation, small language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

inproceedings{06Zavaleta24,
author = {Angela Zavaleta Bernuy and Naaz Sibia and Pan Chen and Jessica Jia-Ni Xu and Elexandra Tran and Runlong Ye and Viktoria Pammer-Schindler and Andrew Petersen and Joseph Jay Williams and Michael Liut},
title = {Does the Medium Matter? An Exploration of Voice-Interaction for Self-Explanations},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661596},
doi = {10.1145/3643834.3661596},
abstract = {This research evaluates voice-based self-explanations as a pedagogical tool in preparation for lectures, assesses user preferences between voice and text, and derives design insights. We report two studies: Study 1, a quasi-experimental field study, with 247 participants divided into voice-based (N = 83), text-based (N = 81), and choice (N = 83) conditions. Study 2 uses semi-structured interviews (N = 16) to explore perceptions of the interaction paradigms in-depth. Results from the first study revealed a general preference for text, though voice users produced longer responses and more topic-related keywords. Over time, the preference for voice increased among students, from 10\% to 46\%, when given a choice. Study 2 suggested that factors like social presence contribute to hesitance toward voice-based explanations, with a cognitive load, self-confidence, and performance anxiety also influencing medium preferences. Our findings highlight design recommendations and demonstrate the potential of voice-based self-explanations in educational settings, indicating that mixed interfaces might better meet diverse needs.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {86–101},
numpages = {16},
keywords = {Active Learning, Explanation Prompts, Long-Term Memory, Self-Explanations, Student Performance, Text Explanations, Voice Explanations, Voice-based Interaction},
location = {IT University of Copenhagen, Denmark},
series = {DIS '24}
}

@INPROCEEDINGS{06Sheinman24,
  author={Michael Sheinman Orenstrakh and Oscar Karnalim and Carlos Aníbal Suárez and Michael Liut},
  booktitle={2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={Detecting LLM-Generated Text in Computing Education: Comparative Study for ChatGPT Cases}, 
  year={2024},
  volume={},
  number={},
  pages={121-126},
  url={https://doi.org/10.1109/COMPSAC61105.2024.00027},
  keywords={Measurement;Accuracy;Plagiarism;Large language models;Education;Detectors;Chatbots;Large Language Models;ChatGPT;GPT;AI Detectors;Plagiarism;Academic Integrity},
  doi={10.1109/COMPSAC61105.2024.00027}
}

inproceedings{04Sibia24,
author = {Naaz Sibia and Angela Zavaleta Bernuy and Elexandra Tran and Jessica Jia-Ni Xu and Joseph Jay Williams and Andrew Petersen and Michael Liut},
title = {Exploring Self-Explanations in a Flipped Database Course},
year = {2024},
isbn = {9798400706783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663649.3664374},
doi = {10.1145/3663649.3664374},
abstract = {Self-explanations show promise for engaging students with preparatory materials, yet research into the types of self-explanations submitted in computing is limited. This paper examines student perceptions of self-explanation prompts in a flipped databases course, building on existing research that highlights the advantages of self-explanations in such contexts. We present our findings on students’ perceptions of the utility of self-explanation prompts and analyze
the nature of the explanations generated across distinct topics. The results suggest that self-explanations not only facilitate a deeper understanding of the subject matter but also promote the discovery of new connections and examples through rewording explanations. Furthermore, errors within self-explanations offer valuable insights for the early identification of misconceptions.},
booktitle = {Proceedings of the 3rd International Workshop on Data Systems Education: Bridging Education Practice with Education Research},
pages = {20–26},
numpages = {7},
keywords = {Active Learning, Flipped Databases Course, Self-Explanations},
location = {Santiago, AA, Chile},
series = {DataEd '24}
}

@inproceedings{05Musabirov24,
author = {Ilya Musabirov and Angela Zavaleta Bernuy and Pan Chen and Michael Liut and Joseph Williams},
title = {Opportunities for Adaptive Experiments to Enable Continuous Improvement in Computer Science Education},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660659},
doi = {10.1145/3660650.3660659},
abstract = {Randomized A/B comparisons of alternative pedagogical strategies or other course improvements could provide useful empirical evidence for instructor decision-making. However, traditional experiments do not provide a straightforward pathway to rapidly utilize data, increasing the chances that students in an experiment experience the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might aid continuous course improvement. In adaptive experiments, data is analyzed and utilized as different conditions are deployed to students. This can be achieved using machine learning algorithms to identify which actions are more beneficial in improving students’ learning experiences and outcomes. These algorithms can then dynamically deploy the most effective conditions in subsequent interactions with students, resulting in better support for students’ needs. We illustrate this approach with a case study that provides a side-by-side comparison of traditional and adaptive experiments on adding self-explanation prompts in online homework problems in a CS1 course. This work paves the way for exploring the importance of adaptive experiments in bridging research and practice to achieve continuous improvement in educational settings.},
booktitle = {The 26th Western Canadian Conference on Computing Education},
articleno = {4},
numpages = {7},
keywords = {Thompson sampling, adaptive experimentation, continuous improvement, multiarmed bandit},
location = {<conf-loc>, <city>Kelowna</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {WCCCE '24}
}

@inproceedings{05Zavaleta24,
author = {Angela Zavaleta Bernuy and Andrew Chung and Alana Hodge and Ayesha Tayyiba and Michael Liut and Andrew Petersen},
title = {Student Transitions Through an Entire Computing Program},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660661},
doi = {10.1145/3660650.3660661},
abstract = {While the challenges experienced by first-year computing students have been well studied, little work has explored the transitions in disciplinary participation and challenges experienced by upper-years. This study explores how students’ needs and challenges evolve through a computing degree. We collected the experiences of first to final-year undergraduate computing students through surveys and interviews. We organized these experiences into themes that we compare against previous literature and illustrate with quotes. Upper-year students perceive changes in (a) levels of support and (b) the kinds of challenges they experience as they progress through the program. Second-year students feel pressured by the increasing difficulty of courses. This pressure increases through the third year as students begin to perceive a need to find employment. The experiences of our students suggest the need to better support the middle years of academic programs. Students in the first year are well-supported in their university transition, but students in the middle are often left to find their way as they develop a deeper understanding of their desired place in the field.},
booktitle = {The 26th Western Canadian Conference on Computing Education},
articleno = {5},
numpages = {7},
keywords = {CS1, student experience, transition, upper-year},
location = {<conf-loc>, <city>Kelowna</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {WCCCE '24}
}

@inproceedings{05Reza24,
author = {Mohi Reza and Nathan M Laundry and Ilya Musabirov and Peter Dushniku and Zhi Yuan "Michael" Yu and Kashish Mittal and Tovi Grossman and Michael Liut and Anastasia Kuzminykh and Joseph Jay Williams},
title = {ABScribe: Rapid Exploration \& Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641899},
doi = {10.1145/3613904.3641899},
abstract = {Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art Large Language Models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new variations without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration and organization of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly modify variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text fields for rapid in-place comparisons using mouse-over interactions on a popup toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions of the revision process (d = 2.41, p < 0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {1042},
numpages = {18},
keywords = {datasets, gaze detection, neural networks, text tagging},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{05Liut24,
author = {Bhattacharjee, Ananya and Zeng, Yuchen and Xu, Sarah Yi and Kulzhabayeva, Dana and Ma, Minyi and Kornfield, Rachel and Ahmed, Syed Ishtiaque and Mariakakis, Alex and Czerwinski, Mary P and Kuzminykh, Anastasia and Liut, Michael and Williams, Joseph Jay},
title = {Understanding the Role of Large Language Models in Personalizing and Scaffolding Strategies to Combat Academic Procrastination},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642081},
doi = {10.1145/3613904.3642081},
abstract = {Traditional interventions for academic procrastination often fail to capture the nuanced, individual-specific factors that underlie them. Large language models (LLMs) hold immense potential for addressing this gap by permitting open-ended inputs, including the ability to customize interventions to individuals' unique needs. However, user expectations and potential limitations of LLMs in this context remain underexplored. To address this, we conducted interviews and focus group discussions with 15 university students and 6 experts, during which a technology probe for generating personalized advice for managing procrastination was presented. Our results highlight the necessity for LLMs to provide structured, deadline-oriented steps and enhanced user support mechanisms. Additionally, our results surface the need for an adaptive approach to questioning based on factors like busyness. These findings offer crucial design implications for the development of LLM-based tools for managing procrastination while cautioning the use of LLMs for therapeutic guidance.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {15},
numpages = {18},
keywords = {ChatGPT, Education, GPT-4, Large Language Models, Personalized Reflections, Procrastination},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

@inproceedings{03McGill24,
author = {Monica M. McGill and Sarah Heckman and Michael Liut and Ismaila Temitayo Sanusi and Claudia Szabo},
title = {Unlocking Excellence in Educational Research: Guidelines for High-Quality Research that Promotes Learning for All},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633402},
doi = {10.1145/3626253.3633402},
abstract = {While there are multiple standards bodies that define characteristics of high-quality, there are limited guidelines on conducting equity-enabling research, particularly in the context of high quality and in computing education. As part of an ACM ITiCSE Working Group in 2023, we engaged in a concept analysis and structured literature review to identify high-impact practices for conducting both high-quality and equity-enabling education research. As a result of this work, we produced a set of guidelines across each major phase of research that integrates characteristics of high-quality education research with those that are necessary for producing research that is designed to honor and meet the needs of various subgroups of learners. Special emphasis is given to the role that the researcher plays in shaping the research based upon how the researcher's lived experiences, perspectives, and training influences their work. During this special session, we will review each set of guidelines and engage attendees in reflection and discussion of them and how they can use the guidelines to enhance their education research.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1871–1872},
numpages = {2},
keywords = {computing education, cs for all, equity, guidelines, high-quality, post-secondary, primary, research, secondary},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{03Bui24,
author = {Giang Bui and Nicholas Susanto and Naaz Sibia and Angela Zavaleta Bernuy and Michael Liut and Andrew Petersen},
title = {Do Hints Enhance Learning in Programming Exercises? Exploring Students' Problem-Solving and Interactions},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635563},
doi = {10.1145/3626253.3635563},
abstract = {Asking for help (help-seeking) is a recognized and effective problem-solving strategy. This study investigates students' interaction with on-demand hints (automated hints requested by students) and assesses their impact on learning progress. We conducted an A/B experiment in a third-year computer science database course, offering hints for selected SQL problems with different hint designs. We collected data on students' code submissions, grades, and hint requests, and we administered a survey to gather feedback and gauge student perception of the hints. Many students accessed hints immediately without attempting the problem first, often requesting multiple hints in quick succession. While students perceived the hints to be valuable, we did not detect an impact on student problem-solving. These insights could inform future studies on the possible impact of students' attitudes toward hints, and how different types of hints might impact uptake and perception of hints.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1578–1579},
numpages = {2},
keywords = {help-seeking behavior, intelligent tutoring systems, on-demand hints, self-regulated learning},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{03Liut24,
author = {Michael Liut and Anna Ly and Jessica Jia-Ni Xu and Justice Banson and Paul Vrbik and Caroline D. Hardin},
title = {"I Didn't Know": Examining Student Understanding of Academic Dishonesty in Computer Science},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630753},
doi = {10.1145/3626252.3630753},
abstract = {In contrast with studies that have identified why students commit academic offences, many educators are familiar with the excuse that an accused student did not know the behavior counted as dishonest. Given the variations in policy and the ways collaboration and code sharing occur in professional and hobbyist spaces, this might be plausible. Mismatches between students' conceptions of academic honesty and course policy can have major consequences, from being kicked out of programs to being too nervous to study with peers. In this work, we investigate what students understand about academic integrity in computer science courses and if there are differences based on university, country, demographic, or online versus in-person courses. We present a study that surveys undergraduate computer science students (N = 1,011) at three universities (Australia, Canada, and the United States of America). The results show that all three institutions take academic integrity seriously, and their students are aware of its importance, but confusion on what is covered under the policies is common. Interestingly, the results also show that course instructors play a huge role as to what students perceive to be a violation of the academic integrity policy at their institution. By understanding student's perspectives on academic integrity, educators can better develop policies and practices that reduce inadvertent and mistaken violations of academic integrity policies.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {757–763},
numpages = {7},
keywords = {academic dishonesty, academic integrity, assessment, cheating, computer science students, computing students, education},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

inproceedings{03Sibia24,
author = {Naaz Sibia and Giang Bui and Bingcheng Wang and Yinyue Tan and Angela Zavaleta Bernuy and Christina Bauer and Joseph Jay Williams and Michael Liut and Andrew Petersen},
title = {Examining Intention to Major in Computer Science: Perceived Potential and Challenges},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630843},
doi = {10.1145/3626252.3630843},
abstract = {This study explores links between attributes of computing students, such as prior programming experience (PE) and gender, with expectations for success and the perception of challenges. Using Expectancy-Value Theory (EVT), we investigate their major intentions and the impact of these factors post-CS1. Data was gathered using surveys at the beginning and end of an introductory programming course, focusing on demographics, expectations of success, and perceptions of challenges. Application status for the computing major was also recorded. Our results revealed that men and students with PE generally perceived greater potential for success and reported facing fewer challenges. In contrast, women and students without PE more often indicated concerns about intellectual ability and perceived challenges less positively. Notably, while gender appears in the preceding results, an intersectional analysis indicates that PE is the central factor. PE is also linked to persistence in the field of computing. Our results further highlight the importance of providing students with opportunities to develop experience, as it can help shape their expectations, perceived challenges, and retention in computing.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1237–1243},
numpages = {7},
keywords = {gender, prior experience, program retention, self-efficacy},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@inproceedings{01Miedema23,
author = {Daphne Miedema and Michael Liut and George H. L. Fletcher and Efthimia Aivaloglou},
title = {“There is no ambiguity on what to return”: Investigating the Prevalence of SQL Misconceptions},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631821},
doi = {10.1145/3631802.3631821},
abstract = {In recent years, database education has been receiving more attention, with research in various directions such as the development of tools for education, the analysis of students’ homework, and the exploration of misconceptions. Misconceptions are mistakes in student reasoning that lead to errors during problem-solving. Recent work has documented misconceptions and errors in SQL. In this study we test the prevalence of several of these misconceptions through a multiple-choice questionnaire, to see if they hold on a larger, more diverse, student population. We found that all misconceptions are held to some extent, with prevalence scores ranging from one to fifty-two percent of the student population. Additionally, we have uncovered previously unidentified areas of struggle, allowing us to identify new misconceptions.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {10},
numpages = {12},
keywords = {Data Systems Education, Databases, Misconceptions, SQL},
location = {<conf-loc>, <city>Koli</city>, <country>Finland</country>, </conf-loc>},
series = {Koli Calling '23}
}

inproceedings{11Zavaleta23,
author = {Angela Zavaleta Bernuy and Runlong Ye and Elexandra Tran and Naaz Sibia and Abhijoy Mandal and Hammad Shaikh and Bogdan Simion and Michael Liut and Andrew Petersen and Joseph Jay Williams},
title = {Do Students Read Instructor Emails? A Case Study of Intervention Email Open Rates},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631813},
doi = {10.1145/3631802.3631813},
abstract = {Email is an important mode of communication because it scales to the largest computing courses and is institutionally supported. Furthermore, regular email communication from instructors has been shown to help set student expectations and encourage participation. As a result, effective email can contribute to emotional engagement, which has been linked to improvements in performance and retention, the latter being a persistent problem in computer science. However, we lack a clear picture of how computing students interact with emails and whether their use aligns with instructors’ expectations. This paper addresses this gap by presenting data on how often CS1 students open instructor emails. We present email engagement data throughout the term for a particular type of email that prompts students to plan to start their homework. Contrary to instructors’ expectations, the rate at which students open emails of this kind does not change significantly over the term. Many students who engage with the emails do so consistently, even after repeated emails throughout the term. The patterns we found illustrate the value of collecting this type of data and informing instructors and researchers about who reads these messages and how often they actually reach students.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {13},
numpages = {12},
keywords = {email engagement, email open rates, instructor communication},
location = {<conf-loc>, <city>Koli</city>, <country>Finland</country>, </conf-loc>},
series = {Koli Calling '23}
}

@inproceedings{12McGill23,
author = {Monica M. McGill and Sarah Heckman and Christos Chytas and Michael Liut and Vera Kazakova and Ismaila Temitayo Sanusi and Selina Marianna Shah and Claudia Szabo},
title = {Conducting Sound, Equity-Enabling Computing Education Research},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633495},
doi = {10.1145/3623762.3633495},
abstract = {Problem. To investigate and identify promising practices in equitable K-12 and tertiary computer science (CS) education, the capacity for education researchers to conduct this research must be rapidly built globally. Simultaneously, concerns have arisen over the last few years about the quality of research that is being conducted and the lack of research that supports teaching all students computing.Research Question. Our research question for our study was: In what ways can existing research standards and practices inform methodologically sound, equity-enabling computing education research?Methodology. We conducted a concept analysis using existing research and various standards (e.g. European Educational Research Association, Australian Education Research Organisation, American Psychological Association). We then synthesised key features in the context of equity-focused K-12 computing education research.Findings. We present a set of guidelines for general research design that takes into account best practices across the standards that are infused with equity-enabling research practices.Implications. Our guidelines will directly impact future equitable computing education research by providing guidance on conducting high-quality research such that the findings can be aggregated and impact future policy with evidence-based results. Because we have crafted these guidelines to be broadly applicable across a variety of settings, we believe that they will be useful to researchers operating in a variety of contexts.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {30–56},
numpages = {27},
keywords = {computer science education research, computing education, equity, evidence, high quality, k-12, post-secondary, primary, research, secondary, standards, tertiary},
location = {Turku, Finland},
series = {ITiCSE-WGR '23}
}

inproceedings{07Bernuy23,
author = {Zavaleta Bernuy, Angela and Ly, Anna and Harrington, Brian and Liut, Michael and Sharmin, Sadia and Zhang, Lisa and Petersen, Andrew},
title = {"I Am Not Enough": Impostor Phenomenon Experiences of University Students},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588779},
doi = {10.1145/3587102.3588779},
abstract = {Recent work has confirmed that computing students experience the Imposter Phenomenon (IP) at higher rates than reported in other disciplines. However, no work has examined what aspects of the university computing experience might lead to a higher rate of IP experiences. We aim to illustrate the IP experiences students have, identify common sources of these experiences, and document the effects of these experiences and how students respond to them. We asked undergraduate students to share recent experiences that illustrate their experiences with the IP. We conducted an inductive thematic analysis on these open-ended responses, resulting in a set of inter-connected themes. A significant fraction of students related stories about making comparisons with peers or observing peer behaviour that made them question their abilities. Students also spoke about holding unrealistic expectations learned from their peers or imposed by the environment. These experiences may be particularly acute for minority-affiliated students who may come to feel they do not belong. Ultimately, these IP experiences can lead to a loss of motivation or a cycle of failure that leads students to leave computing. The central role social comparisons play in IP experiences suggests that it is particularly important to foster communities where opportunities for comparison are reduced and where realistic expectations are explicitly set.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {313–319},
numpages = {7},
keywords = {impostor phenomenon, qualitative, IP, survey},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

inproceedings{07Sibia23,
author = {Sibia, Naaz and Zavaleta Bernuy, Angela and Williams, Joseph Jay and Liut, Michael and Petersen, Andrew},
title = {Student Usage of Q\&A Forums: Signs of Discomfort?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588842},
doi = {10.1145/3587102.3588842},
abstract = {Q\&A forums are widely used in large classes to provide scalable support. In addition to offering students a space to ask questions, these forums aim to create a community and promote engagement. Prior literature suggests that the way students participate in Q\&A forums varies and that most students do not actively post questions or engage in discussions. Students may display different participation behaviours depending on their comfort levels in the class. This paper investigates students' use of a Q\&A forum in a CS1 course. We also analyze student opinions about the forum to explain the observed behaviour, focusing on students' lack of visible participation (lurking, anonymity, private posting). We analyzed forum data collected in a CS1 course across two consecutive years and invited students to complete a survey about perspectives on their forum usage. Despite a small cohort of highly engaged students, we confirmed that most students do not actively read or post on the forum. We discuss students' reasons for the low level of engagement and barriers to participating visibly. Common reasons include fearing a lack of knowledge and repercussions from being visible to the student community.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {33–39},
numpages = {7},
keywords = {confidence, Q\&A forums, gender, anonymity},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{03Bui23,
author = {Giang Bui and Naaz Sibia and Angela Zavaleta Bernuy and Michael Liut and Andrew Petersen},
title = {Prior Programming Experience: A Persistent Performance Gap in CS1 and CS2},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569752},
doi = {10.1145/3545945.3569752},
abstract = {Previous work has reported on the advantageous effects of prior experience in CS1, but it remains unclear whether these effects fade over a sequence of introductory programming courses. Furthermore, while student perceptions suggest that prior experience remains important, studies have reported that a student's expectation of their performance is a more accurate predictor of outcome. We aim to confirm if prior experience (formal or informal) provides short-term and long-term advantages in computing courses or if the advantage fades. Furthermore, we explore whether the expectation of performance is a more accurate predictor of student success than informal and formal prior experience. To explore these questions, we deployed surveys in a CS1 course to gauge students' level of prior experience in programming, prediction of final exam grades, and self-efficacy to succeed in university. Grades from CS1 and CS2 were also collected. We observed a persistent (1-letter grade) gap between the performance of students with no prior experience and those with any experience, but we did not observe a noteworthy gap when comparing student performance based on formal or informal experience. We also observed differences in self-efficacy and retention rates between different levels of prior experience. Lastly, we confirm that success in CS1 can be better reflected and predicted by some controllable factors, such as students' perceptions of ability.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {889–895},
numpages = {7},
keywords = {cs2, prior experience, self-efficacy, confidence, cs1, prediction},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{06Liut22,
author = {Naaz Sibia and Michael Liut},
title = {The Positive Effects of Using Reflective Prompts in a Database Course},
year = {2022},
isbn = {9781450393508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531072.3535323},
doi = {10.1145/3531072.3535323},
abstract = {Motivation: Prior literature has identified student reflections as a way to encourage students to express their thoughts in a structured and focused manner. Objectives: Our goal is to examine the impact of reflections in a third year database systems course, which employs an active learning approach and classroom environment. Specifically, we are interested in seeing whether reflecting on key concepts covered in a preparatory component before lecture had an impact on student’s immediate and long-term performance. Methods: Students were divided into two groups, and asked to reflect on different topics after watching lecture videos before completing their homework exercises for 3 weeks. Results: We observed that students who reflected on lecture concepts performed better on homework exercises that covered those same concepts than students who did not reflect on those same concepts. Moreover, students who reflected performed better in subsequent assessments than students who did not reflect at all. Implications: Reflection as a part of the preparatory component in flipped classrooms is a useful component in conceptual understanding. Further research and investigation should be pursued into ways of prompting reflection, and assessing this component in database courses.},
booktitle = {1st International Workshop on Data Systems Education},
pages = {32–37},
numpages = {6},
keywords = {Active Learning, Long-Term Memory, Computer Science Education, Reflective Prompts, Databases, Student Performance},
location = {Philadelphia, PA, USA},
series = {DataEd '22}
}

inproceedings{03Harrington22,
author = {Angela Zavaleta Bernuy and Anna Ly and Brian Harrington and Michael Liut and Andrew Petersen and Sadia Sharmin and Lisa Zhang},
title = {Additional Evidence for the Prevalence of the Impostor Phenomenon in Computing},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499282},
doi = {10.1145/3478431.3499282},
abstract = {Motivation Despite the widespread belief that computing practitioners frequently experience the Imposter Phenomenon (IP), little formal work has measured the prevalence of IP in the computing community despite its negative effect on achievement.Objectives This study aims to replicate recent work that has suggested that IP experiences are widespread in computing students and to extend that work by exploring the relationship between the IP, progress in the program, and ethnic identity.Methods A survey with several demographic questions (gender, ethnicity, international status, and year of study) and Clance's IP scale (CIPS) was deployed to students in post-secondary computing courses. Correlations between demographic factors and CIPS scores were evaluated, and a linear model was constructed to explore the interaction between demographic factors of interest.Results We reaffirm that a high proportion of CS students meet the IP diagnostic criteria and that women report higher CIPS scores than men. We also present evidence that Asian students with domestic and international status report different levels of IP experiences.Discussion These findings highlight the importance – to educators at all levels – of cultivating belonging in computing communities.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education},
pages = {654–660},
numpages = {7},
keywords = {imposter syndrome, impostor phenomenon, belonging},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{03Liut22,
author = {Pan Chen and Naaz Sibia and Angela Zavaleta Bernuy and Michael Liut and Joseph Jay Williams},
title = {Investigating the Impact of Voice Response Options in Surveys},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478432.3499087},
doi = {10.1145/3478432.3499087},
abstract = {With the widespread usage of mobile devices, users can now choose to provide input through voice or text. As researchers frequently ask students open-ended questions, we want to explore a natural mode to obtain better feedback in surveys. This study details a preliminary study demonstrating the importance of allowing students to choose between voice or text input to respond to surveys. A survey with several open-ended questions was deployed in a CS1 course. Correlations between the gender of the respondent and their method of responding were evaluated. We found that voice responses tended to be longer and preferred more by females relative to male students.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1124},
numpages = {1},
keywords = {surveys, open-ended questions, voice responses, cs1},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{11Liut20,
author = {Muyu Wang and Naaz Sibia and Ilir Dema and Michael Liut and Carlos An\'{\i}bal Su\'{a}rez},
title = {Building a Better SQL Automarker for Database Courses},
year = {2021},
isbn = {9781450384889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488042.3489970},
doi = {10.1145/3488042.3489970},
abstract = {This work introduces and demonstrates the viability of a novel SQL automarking tool (“SQAM”) that: (1) provides a fair grade to the student, one which matches the student’s effort and understanding of the course material, and (2) to provide personalized feedback, allowing the student to remain engaged in the material and learn from their mistakes while still being in that headspace. Additionally, we strive to ensure that our tool maintains the same standards (grade and feedback) that a highly qualified member of teaching staff would produce, so we compare and contrast our automarker’s results to that of teaching assistants over several historic offerings of the same database course at a large research intensive public institution, while reducing the grading time, thus enabling the teaching staff to channel more time into instruction. Furthermore, we describe SQAM’s design and our model which applies the aggregate result of four different string similarity metrics to compute solution similarity in conjunction with our discretization process to fairly evaluate a student’s submission. Our results show that SQAM produces very similar grades to those which were historically given by teaching assistants. },
booktitle = {21st Koli Calling International Conference on Computing Education Research},
articleno = {37},
numpages = {3},
keywords = {SQL Automarking, Partial Marking, String Regularities, String Similarity, Database Course Tools, Student Feedback Enhancement},
location = {Joensuu, Finland},
series = {Koli Calling '21}
}

@inproceedings{11ly21,
author = {Anna Ly and Jack Parkinson and Quintin Cutts and Michael Liut and Andrew Petersen},
title = {Spatial Skills and Demographic Factors in CS1},
year = {2021},
isbn = {9781450384889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488042.3488049},
doi = {10.1145/3488042.3488049},
abstract = {Motivation Prior studies have established that training spatial skills may improve outcomes in computing courses. Very few of these studies have, however, explored the impact of spatial skills training on women or examined its relationship with other factors commonly explored in the context of academic performance, such as socioeconomic background and self-efficacy. Objectives In this study, we report on a spatial skills intervention deployed in a computer programming course (CS1) in the first year of a post-secondary program. We explore the relationship between various demographic factors, course performance, and spatial skills ability at both the beginning and end of the term. Methods Data was collected using a combination of demographic surveys, existing self-efficacy and CS1 content instruments, and the Revised PVST:R spatial skills assessment. Spatial skills were evaluated both at the beginning of the term and at the end, after spatial skills training was provided. Results While little evidence was found to link spatial skills to socioeconomic status or self-efficacy, both gender identity and previous experience in computing were found to be correlated to spatial skills ability at the start of the course. Women initially recorded lower spatial skills ability, but after training, the distribution of spatial skills scores for women approached that of men. Discussion These findings suggest that, if offered early enough, spatial skills training may be able to remedy some differences in background that impact performance in computing courses. },
booktitle = {21st Koli Calling International Conference on Computing Education Research},
articleno = {4},
numpages = {10},
keywords = {retention, spatial skills, socioeconomic status, CS1, gender},
location = {Joensuu, Finland},
series = {Koli Calling '21}
}

@article{12parkinson21,
author = {Jack Parkinson and Ryan Bockmon and Quintin Cutts and Michael Liut and Andrew Petersen and Sheryl Sorby},
title = {Practice Report: Six Studies of Spatial Skills Training in Introductory Computer Science},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3494574},
doi = {10.1145/3494574},
journal = {ACM Inroads},
month = {nov},
pages = {18–29},
numpages = {12}
}

@inproceedings{10ly2021revisiting,
  title={Revisiting Syntax Exercises in CS1},
  author={Anna Ly and John Edwards and Michael Liut and Andrew Petersen},
  booktitle={Proceedings of the 22st Annual Conference on Information Technology Education},
  pages={9--14},
  year={2021},
  url = {https://doi.org/10.1145/3450329.3476855},
  doi = {10.1145/3450329.3476855},
}

inproceedings{03Zhang21,
author = {Larry Yueli Zhang and Andrew K. Petersen and Michael Liut and Bogdan Simion and Furkan Alaca},
title = {A Multi-Course Report on the Experience of Unplanned Online Exams},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432515},
doi = {10.1145/3408877.3432515},
abstract = {We report our experience of preparing and conducting unplanned online exams in the unique half-physical, half-virtual semester of Winter 2020. The report covers four courses in a large university's computer science program, ranging from first-year to third-year. With the data generated by students taking both in-person and online exams in multiple courses, we perform analyses to evaluate the validity of the online exams (especially the unproctored ones) as an assessment of student understanding. With the fine-grained student activity data provided by the online exam platform, we are also able to investigate the patterns in student exam-taking behaviours and their correlations with student performance on the exam. In addition, we share, in detail, the tips and lessons that were learned throughout the process of designing, implementing, and hosting the online exams.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {17–23},
numpages = {7},
keywords = {student behaviour, online exams, assessment},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{08Liut20,
 author = {Arnaud Deza and Haocheng Hu and Vaishvik Maisuria and Michael Liut and Andrew Petersen and Bogdan Simion},
 title = {Using Discussion Board Data to Hire Teaching Assistants},
 booktitle = {Proceedings of the 6th SPLICE Workshop at L@S},
 year = {2020},
 numpage = {6},
 url = {https://cssplice.github.io/LAS20/proc/SPLICE_2020_LS_paper_9.pdf},
}

@inproceedings{06Liut20,
author = {Simon and Oscar Karnalim and Judy Sheard and Ilir Dema and Amey Karkare and Juho Leinonen and Michael Liut and Ren\'{e}e McCauley},
title = {Choosing Code Segments to Exclude from Code Similarity Detection},
year = {2020},
isbn = {9781450382939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437800.3439201},
doi = {10.1145/3437800.3439201},
booktitle = {Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {1–19},
numpages = {19},
keywords = {collusion, academic integrity, plagiarism, code similarity detection},
location = {Trondheim, Norway},
series = {ITiCSE-WGR '20}
}
