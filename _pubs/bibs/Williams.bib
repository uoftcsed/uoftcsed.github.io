@inproceedings{03Zavaleta24,
author = {Angela Zavaleta Bernuy and Runlong Ye and Naaz Sibia and Rohita Nalluri and Joseph Jay Williams and Andrew Petersen and Eric Smith and Bogdan Simion and Michael Liut},
title = {Student Interaction with Instructor Emails in Introductory and Upper-Year Computing Courses},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630930},
doi = {10.1145/3626252.3630930},
abstract = {In computing courses, instructor involvement and social comfort are vital for resilience and belonging. We examine engagement with instructor emails aimed at strengthening the connection with students. We sent weekly emails from instructors to first- and upper-year computing students. These emails included reminders for the assignments due each week. Half of the students received reminders embedded in an informal message that contained approachable wording and relevant current course events, while the rest received a list of precise deadlines. This text had no emotional engagement from the instructor. We collected and analyzed email access and link click rates, along with student survey responses about email preferences and engagement. We found that first-year students had lower email access and link click rates than upper-year students. While we did not find differences in first-year engagement based on the type of email, upper-year students appeared to be more engaged when receiving the intentionally informal version of the email. Understanding the message preferences of computing students can enhance instructor messaging and improve engagement. Strategies should be explored to boost first-year student engagement, while the higher engagement among upper-year students underscores the importance of instructor support in advanced courses.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1477–1483},
numpages = {7},
keywords = {cs2, email engagement, upper-year students},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

@inproceedings{03Sibia24,
author = {Naaz Sibia and Giang Bui and Bingcheng Wang and Yinyue Tan and Angela Zavaleta Bernuy and Christina Bauer and Joseph Jay Williams and Michael Liut and Andrew Petersen},
title = {Examining Intention to Major in Computer Science: Perceived Potential and Challenges},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630843},
doi = {10.1145/3626252.3630843},
abstract = {This study explores links between attributes of computing students, such as prior programming experience (PE) and gender, with expectations for success and the perception of challenges. Using Expectancy-Value Theory (EVT), we investigate their major intentions and the impact of these factors post-CS1. Data was gathered using surveys at the beginning and end of an introductory programming course, focusing on demographics, expectations of success, and perceptions of challenges. Application status for the computing major was also recorded. Our results revealed that men and students with PE generally perceived greater potential for success and reported facing fewer challenges. In contrast, women and students without PE more often indicated concerns about intellectual ability and perceived challenges less positively. Notably, while gender appears in the preceding results, an intersectional analysis indicates that PE is the central factor. PE is also linked to persistence in the field of computing. Our results further highlight the importance of providing students with opportunities to develop experience, as it can help shape their expectations, perceived challenges, and retention in computing.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1237–1243},
numpages = {7},
keywords = {gender, prior experience, program retention, self-efficacy},
location = {<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>},
series = {SIGCSE 2024}
}

inproceedings{11Zavaleta23,
author = {Angela Zavaleta Bernuy and Runlong Ye and Elexandra Tran and Naaz Sibia and Abhijoy Mandal and Hammad Shaikh and Bogdan Simion and Michael Liut and Andrew Petersen and Joseph Jay Williams},
title = {Do Students Read Instructor Emails? A Case Study of Intervention Email Open Rates},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631813},
doi = {10.1145/3631802.3631813},
abstract = {Email is an important mode of communication because it scales to the largest computing courses and is institutionally supported. Furthermore, regular email communication from instructors has been shown to help set student expectations and encourage participation. As a result, effective email can contribute to emotional engagement, which has been linked to improvements in performance and retention, the latter being a persistent problem in computer science. However, we lack a clear picture of how computing students interact with emails and whether their use aligns with instructors’ expectations. This paper addresses this gap by presenting data on how often CS1 students open instructor emails. We present email engagement data throughout the term for a particular type of email that prompts students to plan to start their homework. Contrary to instructors’ expectations, the rate at which students open emails of this kind does not change significantly over the term. Many students who engage with the emails do so consistently, even after repeated emails throughout the term. The patterns we found illustrate the value of collecting this type of data and informing instructors and researchers about who reads these messages and how often they actually reach students.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {13},
numpages = {12},
keywords = {email engagement, email open rates, instructor communication},
location = {<conf-loc>, <city>Koli</city>, <country>Finland</country>, </conf-loc>},
series = {Koli Calling '23}
}

@inproceedings{07Sibia23,
author = {Naaz Sibia and Angela Zavaleta Bernuy and Joseph Jay Williams and Michael Liut and Andrew Petersen},
title = {Student Usage of Q\&A Forums: Signs of Discomfort?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588842},
doi = {10.1145/3587102.3588842},
abstract = {Q\&A forums are widely used in large classes to provide scalable support. In addition to offering students a space to ask questions, these forums aim to create a community and promote engagement. Prior literature suggests that the way students participate in Q\&A forums varies and that most students do not actively post questions or engage in discussions. Students may display different participation behaviours depending on their comfort levels in the class. This paper investigates students' use of a Q\&A forum in a CS1 course. We also analyze student opinions about the forum to explain the observed behaviour, focusing on students' lack of visible participation (lurking, anonymity, private posting). We analyzed forum data collected in a CS1 course across two consecutive years and invited students to complete a survey about perspectives on their forum usage. Despite a small cohort of highly engaged students, we confirmed that most students do not actively read or post on the forum. We discuss students' reasons for the low level of engagement and barriers to participating visibly. Common reasons include fearing a lack of knowledge and repercussions from being visible to the student community.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {33–39},
numpages = {7},
keywords = {confidence, Q\&A forums, gender, anonymity},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{09Ye22,
author = {Runlong Ye and Pan Chen and Yini Mao and Angela Wang-Lin and Hammad Shaikh and Angela Zavaleta Bernuy and Joseph Jay Williams},
title = {Behavioral Consequences of Reminder Emails on Students’ Academic Performance: A Real-World Deployment},
year = {2022},
isbn = {9781450393911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537674.3554740},
doi = {10.1145/3537674.3554740},
abstract = {Post-secondary institutions have experienced a continuous trend of student procrastination on course work, thus leading to lower academic performance and potentially worse knowledge retention and other longer-term impacts. Sending reminders about deliverables is a simple approach, but it has the potential to be a valuable tool to mitigate such issues and assist students with time management. This paper will introduce specific processes of conducting such experiments, especially email designs and randomization, to help instructors and researchers conduct similar experiments or field-deploying reminders. To evaluate homework reminder messages, we designed and deployed a real-world randomized A/B experiment at a North American university in a CS1 course where students were randomly assigned to either receive these reminder messages or not. Our findings suggest that students who received the reminder messages have a higher homework completion rate (p &lt; .05) and performed significantly better (p &lt; .01) on the following midterm test than students who did not receive the reminder message. Finally, we discuss how a homework reminder can improve student behaviours, as well as how this type of reminder can be enhanced for future interventions.},
booktitle = {Proceedings of the 23rd Annual Conference on Information Technology Education},
pages = {16–22},
numpages = {7},
keywords = {Factorial design, Randomized control trial, A/B testing, Field deployment, Procrastination, Reminder email, Statistical inference},
location = {Chicago, IL, USA},
series = {SIGITE '22},
comment={<b>Best Paper Award</b>}
}

inproceedings{03Bernuy22,
author = {Zavaleta Bernuy, Angela and Han, Ziwen and Shaikh, Hammad and Zheng, Qi Yin and Lim, Lisa-Angelique and Rafferty, Anna and Petersen, Andrew and Williams, Joseph Jay},
title = {How Can Email Interventions Increase Students’ Completion of Online Homework? A Case Study Using A/B Comparisons},
year = {2022},
isbn = {9781450395731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3506860.3506874},
doi = {10.1145/3506860.3506874},
abstract = {Email communication between instructors and students is ubiquitous, and it could be valuable to explore ways of testing out how to make email messages more impactful. This paper explores the design space of using emails to get students to plan and reflect on starting weekly homework earlier. We deployed a series of email reminders using randomized A/B comparisons to test alternative factors in the design of these emails, providing examples of an experimental paradigm and metrics for a broader range of interventions. We also surveyed and interviewed instructors and students to compare their predictions about the effectiveness of the reminders with their actual impact. We present our results on which seemingly obvious predictions about effective emails are not borne out, despite there being evidence for further exploring these interventions, as they can sometimes motivate students to attempt their homework more often. We also present qualitative evidence about student opinions and behaviours after receiving the emails, to guide further interventions. These findings provide insight into how to use randomized A/B comparisons in everyday channels such as emails, to provide empirical evidence to test our beliefs about the effectiveness of alternative design choices. },
booktitle = {LAK22: 12th International Learning Analytics and Knowledge Conference},
pages = {107–118},
numpages = {12},
keywords = {Randomized experiments, Reminder, Embedded experimentation, Procrastination, A/B comparisons},
location = {Online, USA},
series = {LAK22}
}

InProceedings{06Zavaleta21,
author={Angela Zavaleta-Bernuy, Qi Yin Zheng, Hammad Shaikh, Jacob Nogas, Anna Rafferty, Andrew Petersen, Joseph Jay Williams},
editor="Roll, Ido
and McNamara, Danielle
and Sosnovsky, Sergey
and Luckin, Rose
and Dimitrova, Vania",
title="Using Adaptive Experiments to Rapidly Help Students",
booktitle="Artificial Intelligence in Education",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="422--426",
abstract="Adaptive experiments can increase the chance that current students obtain better outcomes from a field experiment of an instructional intervention. In such experiments, the probability of assigning students to conditions changes while more data is being collected, so students can be assigned to interventions that are likely to perform better. Digital educational environments lower the barrier to conducting such adaptive experiments, but they are rarely applied in education. One reason might be that researchers have access to few real-world case studies that illustrate the advantages and disadvantages of these experiments in a specific context. We evaluate the effect of homework email reminders in students by conducting an adaptive experiment using the Thompson Sampling algorithm and compare it to a traditional uniform random experiment. We present this as a case study on how to conduct such experiments, and we raise a range of open questions about the conditions under which adaptive randomized experiments may be more or less useful.",
isbn="978-3-030-78270-2",
url={https://link.springer.com/chapter/10.1007/978-3-030-78270-2_75},
doi={10.1007/978-3-030-78270-2_75}
}

@inproceedings{06Price21,
author = {Thomas W. Price and Samiha Marwan and Joseph Jay Williams},
title = {Exploring Design Choices in Data-Driven Hints for Python Programming Homework},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460159},
doi = {10.1145/3430895.3460159},
abstract = {Students often struggle during programming homework and may need help getting started or localizing errors. One promising and scalable solution is to provide automated programming hints, generated from prior student data, which suggest how a student can edit their code to get closer to a solution, but little work has explored how to design these hints for large-scale, real-world classroom settings, or evaluated such designs. In this paper, we present CodeChecker, a system which generates hints automatically using student data, and incorporates them into an existing CS1 online homework environment, used by over 1000 students per semester. We present insights from survey and interview data, about student and instructor perceptions of the system. Our results highlight affordances and limitations of automated hints, and suggest how specific design choices may have impacted their effectiveness.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {283–286},
numpages = {4},
keywords = {automated programming hints, computing education},
location = {Virtual Event, Germany},
series = {L@S '21}
}

@inproceedings{06Reza21,
author = {Mohi Reza and Juho Kim and Ananya Bhattacharjee and Anna N. Rafferty and Joseph Jay Williams},
title = {The MOOClet Framework: Unifying Experimentation, Dynamic Improvement, and Personalization in Online Courses},
year = {2021},
isbn = {9781450382151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430895.3460128},
doi = {10.1145/3430895.3460128},
abstract = {How can educational platforms be instrumented to accelerate the use of research to improve students' experiences? We show how modular components of any educational interface - e.g. explanations, homework problems, even emails - can be implemented using the novel MOOClet software architecture. Researchers and instructors can use these augmented MOOClet components for: (1) Iterative Cycles of Randomized Experiments that test alternative versions of course content; (2) Data-Driven Improvement using adaptive experiments that rapidly use data to give better versions of content to future students, on the order of days rather than months. A MOOClet supports both manual and automated improvement using reinforcement learning; (3) Personalization by delivering alternative versions as a function of data about a student's characteristics or subgroup, using both expert-authored rules and data mining algorithms. We provide an open-source web service for implementing MOOClets (www.mooclet.org) that has been used with thousands of students. The MOOClet framework provides an ecosystem that transforms online course components into collaborative micro-laboratories, where instructors, experimental researchers, and data mining/machine learning researchers can engage in perpetual cycles of experimentation, improvement, and personalization.},
booktitle = {Proceedings of the Eighth ACM Conference on Learning @ Scale},
pages = {15–26},
numpages = {12},
keywords = {A/B comparisons, randomized experiments, massive open online courses, dynamic improvement, multi-armed bandits, education technology, personalization},
location = {Virtual Event, Germany},
series = {L@S '21}
}

inproceedings{03Petersen21-2,
author = {Angela Zavaleta Bernuy and Qi Yin Zheng and Hammad Shaikh and Andrew Petersen and Joseph Jay Williams},
title = {Investigating the Impact of Online Homework Reminders Using Randomized A/B Comparisons},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432427},
doi = {10.1145/3408877.3432427},
abstract = {Procrastination by students may lead to adverse outcomes such as a focus on completion rather than learning or even a failure to complete learning tasks. One common method for motivating students and reducing procrastination is to send reminders with hints and study strategies, but it's not clear if these messages are effective or when is the best time to send them. Randomized A/B comparisons could be used to try different reminders or alternative ideas about how best to get students to start work earlier and, crucially, to measure the impact of these interventions on behaviour. This paper describes an A/B comparison of reminder emails set in a large CS1 course at a research-focused North American university. We found evidence that the email interventions caused a higher proportion of students to attempt the online homework but did not see evidence that these particular emails got students to start early, irrespective of changes to the timing of the reminder. More broadly, these findings illustrate how to use A/B comparisons in educational settings to test ideas about how to help students, and demonstrate the value of using randomized A/B comparisons, even when evaluating actions that seem obviously beneficial, such as reminder emails.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {921–927},
numpages = {7},
keywords = {A/B comparisons, procrastination, reminders, CS1, email},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

inproceedings{03Petersen21,
author = {Jaemarie Solyst and Trisha Thakur and Madhurima Dutta and Yuya Asano and Andrew Petersen and Joseph Jay Williams},
title = {Procrastination and Gaming in an Online Homework System of an Inverted CS1},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432440},
doi = {10.1145/3408877.3432440},
abstract = {Engaged preparation and study in combination with lectures are important for all courses but are particularly critical for online, hybrid, and inverted classrooms. Many instructors use online systems to deliver new course content and exercises, but students often delay assignments or game these systems (e.g., guessing on multiple-choice questions), often to the detriment of their learning. In an inverted CS1 course, many students self-reported high rates of gaming-the-system behavior, so we examine survey data to identify factors that contribute to engagement in these maladaptive behaviours. We supplement that analysis with interview data to gain a deeper understanding of the situation. We also implemented and evaluated a previously reported online intervention aimed at reducing gaming behavior. Unlike prior work, our intervention did not have a significant effect on guessing behavior. We discuss why the factors we identified might explain this result, as well as suggest future work to improve our understanding of gaming behaviours and inform the design of systems that encourage effective learning.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {789–795},
numpages = {7},
keywords = {online learning, inverted classroom, gaming the system, procrastination},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{08Marwan19,
 author = {Samiha Marwan and Joseph Jay Williams and Thomas Price},
 title = {An Evaluation of the Impact of Automated Programming Hints on Performance and Learning},
 booktitle = {Proceedings of the 2019 ACM Conference on International Computing Education Research},
 series = {ICER '19},
 year = {2019},
 isbn = {978-1-4503-6185-9},
 location = {Toronto ON, Canada},
 pages = {61--70},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3291279.3339420},
 doi = {10.1145/3291279.3339420},
 acmid = {3339420},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {block-based programming, computer science education, next-step hints, self-explanation},
} 

@inproceedings{04Williams18,
 author = {Joseph Jay Williams and Anna N. Rafferty and Dustin Tingley and Andrew Ang and Walter S. Lasecki and Juho Kim},
 title = {Enhancing Online Problems Through Instructor-Centered Tools for Randomized Experiments},
 booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {207:1--207:12},
 articleno = {207},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3173574.3173781},
 doi = {10.1145/3173574.3173781},
 acmid = {3173781},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic experimentation, instructional design, online education, randomized experiments},
}

inproceedings{05Zhang19,
 author = {Lisa Zhang and Michelle Craig and Mark Kazakevich and Joseph Jay Williams},
 title = {Experience Report: Mini Guest Lectures in a CS1 Course via Video Conferencing},
 booktitle = {Proceedings of the 1st ACM Global Computing Education Conference},
 year = {2019},
 comment = {To appear.}
}
